---
title: "Assignment 1 -- Experimental Design and Data Analysis"
output: 
  pdf_document:
    includes:
      before_body: title.sty
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 1

The data set birthweight.txt contains the birthweights of 188 newborn babies. We are interested in finding the underlying (population) mean μ of birthweights.

a) Check normality of the data. Compute a point estimate for μ. Derive, assuming normality (irrespective of your conclusion about normality od the data), a bounded 90% confidence interval for μ.

```{r echo=FALSE}
birth = read.table("birthweight.txt",header=TRUE)
birthweight = birth$birthweight
```

Point estimate for μ:
```{r}
mean(birthweight)
```

Check normality with Shapiro-Wilk test:
```{r}
shapiro.test(birthweight)[2]
```
p is non-significant. This implies that h0 is accepted (the data is normally distributed).


b) An expert claims that the mean birthweight is bigger than 2800, verify this claim by using a t-test. What is the outcome of the test if you take α = 0.1? And other values of α?

If we test this hypothesis with α = 0.1 we get:
```{r}
alpha = .1
t.test(birthweight, alternative = "l", conf.level = 1-alpha, mu = 2800)[3]
```

p = .9864, therefore, h0 is accepted. If we try this for alpha = .05, .01, .001, we get the following p values:
```{r}
alpha = .05
t.test(birthweight, alternative = "l", conf.level = 1-alpha, mu = 2800)[3]

alpha = .05

```



c) In the R-output of the test from b), also a confidence interval is given, but why is it different from the confidence interval found in a) and why is it one-sided?

## Exercise 2
We study the power function of the two-sample t-test (see Section 1.9 of Assignment 0). For n=m=30, mu=180, nu=175 and sd=5, generate 1000 samples x=rnorm(n,mu,sd) and y=rnorm(m,nu,sd), and record the 1000 p-values for testing H0: mu=nu. You can evaluate the power (at point nu=175) of this t-test as fraction of p-values that are smaller than 0.05.
```{r}
n=m=30
mu=180
nu=175
sd=5

B=1000
p=numeric(B)
for (b in 1:B) {
  x=rnorm(n,mu,sd);
  y=rnorm(m,nu,sd);
  p[b]=t.test(x,y,var.equal = TRUE)[[3]]}
a <- table(p)
length(a[names(a)<0.05])/length(a)
```
From the 1000 generated samples a fraction of 0.577 samples have p-values lower than 0.05.

a) Set n=m=30, mu=180 and sd=5. Calculate now the power of the t-test for every value of nu in the grid seq(175,185,by=0.25). Plot the power as a function of nu.
```{r}
n=m=30
mu=180
nus=seq(175,185,by=0.25)
sd=5

powers = numeric(length(nus))

B=1000
p=numeric(B)
for (i in 1:length(nus)) {
  nu = nus[i]
  for (b in 1:B) {
    x=rnorm(n,mu,sd);
    y=rnorm(m,nu,sd);
    p[b]=t.test(x,y,var.equal = TRUE)[[3]]}
  a <- table(p)
  powers[i] = length(a[names(a)<0.05])/length(a)
} 
powers_a = powers
nus_a = nus

plot1 = plot(nus_a, powers_a)
```

b) Set n=m=100, mu=180 and sd=5. Repeat the preceding exercise. Add the plot to the preceding plot.
```{r}
n=m=100
mu=180
nus=seq(175,185,by=0.25)
sd=5

powers = numeric(length(nus))

B=1000
p=numeric(B)
for (i in 1:length(nus)) {
  nu = nus[i]
  for (b in 1:B) {
    x=rnorm(n,mu,sd);
    y=rnorm(m,nu,sd);
    p[b]=t.test(x,y,var.equal = TRUE)[[3]]}
  a <- table(p)
  powers[i] = length(a[names(a)<0.05])/length(a)
} 
powers_b = powers
nus_b = nus

# plot2 = plot(nus, powers)
```

c) Set n=m=30, mu=180 and sd=15. Repeat the preceding exercise.
```{r, figures-side, fig.show="hold", out.width="50%"}
n=m=30
mu=180
nus=seq(175,185,by=0.25)
sd=15

powers = numeric(length(nus))

B=1000
p=numeric(B)
for (i in 1:length(nus)) {
  nu = nus[i]
  for (b in 1:B) {
    x=rnorm(n,mu,sd);
    y=rnorm(m,nu,sd);
    p[b]=t.test(x,y,var.equal = TRUE)[[3]]}
  a <- table(p)
  powers[i] = length(a[names(a)<0.05])/length(a)
} 
powers_c = powers
nus_c = nus
```

```{r}
plot(nus_a, powers_a, main="Results 2", xlab="nu", ylab="fraction of power < 0.05", pch=20, col="blue")
lines(nus_a, powers_a, xlab="nu", pch=20, col="blue")
points(nus_b, powers_b, xlab="nu", pch=20, col="red")
lines(nus_b, powers_b, xlab="nu", pch=20, col="red")
points(nus_c, powers_c, xlab="nu", pch=20, col="green")
lines(nus_c, powers_c, xlab="nu", pch=20, col="green")
legend(184, 0.5, legend=c("a", "b", "c"), col=c("blue", "red", "green"), lty=1, cex=0.8)
```
d) Explain your findings.
The graph shows the findings of question a, b and c. It is clearly shown that depending on the

difference between a and b is 30 and 100 samples, respectively.
so more samples in b, result in more fluctuations
so nu 176 gives a power of <0.1 while nu 178 gives a power of approx. 0.7.
mu = 180. so shouldnt nu 176 be a higher power? because further away from 180, so "easier" to detect that mu != nu and thus H0 false, or p < 0.05

difference between a and c is a sd of 5 and 15 respectively.

The findings of question c show that an increase in standard deviation will decrease the power of a t-test. A possible explanation for this could be that because of the higher standard deviation.   


mu is constant at 180.
n and m are same (samples)
test if mu=nu
nu ranges from 175-185

Y-axis shows the fraction of p-values < 0.05. So the amount of times were it was determined that mu does not equal nu.

a = n=m=30, mu=180 and sd=5
b = n=m=100, mu=180 and sd=5
c = n=m=30, mu=180 and sd=15

## Exercise 3
A telecommunication company has entered the market for mobile phones in a new country. The company’s marketing manager conducts a survey of 200 new subscribers for mobile phones. The results of the survey are in the data set telephone.txt, which contains the first month bills X1 , . . . X200 , in euros.

a) Make an appropriate plot of this data set. What marketing advice(s) would you give to the marketing manager? Are there any inconsistencies in the data? If so, try to fix these.

```{r}
telephone_data = read.table("telephone.txt",header=TRUE)

# sorted data of firsth month bills
sort(telephone_data$Bills)
```
If we look at the data, we see several participants that have a first month bill of €0. This does not seem plausible, as the company probably would not give out subscriptions for free. Therefore, we trim the data of zero-values.

```{r}
# row_sub = apply(dd, 1, function(row) all(row !=0 ))
corrected_telephone_data = telephone_data[apply(telephone_data, 1, function(row) all(row !=0 )), ]

par(mfrow=c(2,2))
hist(telephone_data$Bills, xlab = "First month bills (€)", main = "data before correction")
hist(corrected_telephone_data, xlab = "First month bills (€)", main = "data after correction")
boxplot(telephone_data$Bills, ylab = "€")
boxplot(corrected_telephone_data, ylab = "€")
```
We now see that the first bin in the histogram contains a lower frequency due to the removal of zero-values. Based on the boxplots, we see that most participants have relatively low phone bills. Therefore, we might advice the marketing manager to focus on budget subscriptions. 

b) By using a bootstrap test with the test statistic T = median(X1,...,X200), test whether the data telephone.txt stems from the exponential distribution Exp(λ) with some λ from [0.01, 0.1].
```{r}
lamb = .05
hist(corrected_telephone_data, prob=T, ylim = c(0,0.05))
x = seq(0, length(corrected_telephone_data),length=1000)
f = lamb*exp(-lamb*x )
lines(x,f,col="blue",lwd=2)
```

```{r}
t = median(corrected_telephone_data)
B=1000
tstar=numeric(B)
n=length(corrected_telephone_data)
for (i in 1:B){
  xstar=rexp(n, lamb)
  tstar[i]=median(xstar)}
  hist(tstar,prob=T)
  
p_left=sum(tstar<t)/B
p_right=sum(tstar>t)/B
p=2*min(p_left,p_right)
p_left;p_right;p
```

The p-value is .002, therefore, h0 is rejected. This means that the distribution of the data does not come from the exponential distribution with lambda = 1/median(data). 

c) Construct a 95% bootstrap confidence interval for the population median of the sample.
```{r}
B=5000
Tstar=numeric(B)
T1 = median(corrected_telephone_data)

for(i in 1:B) {
  Xstar=sample(corrected_telephone_data,replace=TRUE)
  Tstar[i]=median(Xstar) }
  Tstar05=quantile(Tstar,0.05)
  Tstar95=quantile(Tstar,0.95)
  sum(Tstar<Tstar05)
c(2*T1-Tstar95,2*T1-Tstar05)
```
The 95% bootstrap confidence interval for the population median of the telephone data is [20.7, 36.5] around its median T1=28.9.


d) Assuming X1 , . . . Xn ∼ Exp(λ) and using the central limit theorem for the sample mean, estimate λ and construct again a 95% confidence interval for the population median. Comment on your findings.

First we will repeat the steps from question 1c, but with the sample mean in stead of the sample medians. 

```{r}
B=5000
Tstar=numeric(B)
T1 = median(corrected_telephone_data)

for(i in 1:B) {
  Xstar=sample(corrected_telephone_data,replace=TRUE)
  Tstar[i]=mean(Xstar) 
  }
Tstar05=quantile(Tstar,0.05)
Tstar95=quantile(Tstar,0.95)
sum(Tstar<Tstar05)

c(2*T1-Tstar95,2*T1-Tstar05)
```

<!-- We can translate the median of the samples to lambda since -->
<!-- $median[X] = \frac{ln(2)}{\lambda}$ -->
<!-- meaning: -->
<!-- $\lambda = \frac{ln(2)}{median[X]}$ -->

If we plot the means of the data, we get:
```{r}
hist(Tstar, prob=T)
```

We then check whether the data is normally distributed:
```{r}
shapiro.test(Tstar)
```

Therefore, the CLT applies. We can then obtain an estimate for $\lambda$ with:
$\lambda = \frac{1}{\mu}$

Giving us an estimate of:
```{r}
est_lamb = 1/(mean(Tstar))
est_lamb
```

<!-- misschien dit hieronder weghalen -->
If we plot the exponential distribution with $\lambda = .022$ against our data, we observe:
```{r}
hist(corrected_telephone_data, prob=T, ylim = c(0,0.05))
x = seq(0, length(corrected_telephone_data),length=1000)
f = est_lamb*exp(-est_lamb*x )
lines(x,f,col="blue",lwd=2)
```




e) Using an appropriate test, test the null hypothesis that the median bill is bigger or equal to 40 euro against the alternative that the median bill is smaller than 40 euro. Next, design and perform a test
to check whether the fraction of the bills less than 10 euro is less than 25%.


An appropriate test to test the null hypothesis of a median greater than some value is the Wilcoxon test [leg uit wat de test doet]. 
```{r}
wilcox.test(corrected_telephone_data, mu=40, alternative="less")
```

We get a p-value of 1, implying that we should not reject H0. 

Next, we test whether the fraction of the bills less than 10 euro is less than 25%. We can do this by bootstrapping a 25% confidence interval with a test statistic of 10.

```{r}
B=5000
Tstar=numeric(B)
T1 = 10

for(i in 1:B) {
  Xstar=sample(corrected_telephone_data,replace=TRUE)
  Tstar[i]=sum(Xstar < T1) / length(Xstar)
  }
Tstar95=quantile(Tstar,0.95)
Tstar95
```

Based on the one-sided 95% confidence interval, we can not conclude that 25% of the bills are less than 10 euro.[uitwerken]


## Exercise 4
To study the effect of energy drink a sample of 24 high school pupils were randomized to drinking either a
softdrink or an energy drink after running for 60 meters. After half an hour they were asked to run again.
For both sprints they were asked to sprint as fast they could, and the sprinting time was measured. The
data is given in the file run.txt. [Courtesy class 5E, Stedelijk Gymnasium Leiden, 2010.]

```{r echo=FALSE}
# Load data
run = read.table("run.txt",header=TRUE)
lemo = subset(run, drink == "lemo")
energy = subset(run, drink == "energy")
```

a) Disregarding the type of drink, test whether the run times before drink and after are correlated.

Check for normality:
```{r}
# qqnorm(run$before, main = "Normal Q-Q plot before drink")
# qqnorm(run$after, main = "Normal Q-Q plot after drink")
shapiro.test(run$before)
shapiro.test(run$after)
```
Assumption for normality met since both p-values are larger than 0.05.

```{r}
plot(run$before, run$after, xlab="Run time before drink", ylab="Run time after drink")
cor.test(run$before, run$after,  method = "pearson")
```

Since Pearson's product-moment correlation p < 0.05 we can state that we found significant evidence that a true correlation exists.

b) Test separately, for both the softdrink and the energy drink conditions, whether there is a difference in speed in the two running tasks.

Check for normality:
```{r}
# qqnorm(lemo$before)
shapiro.test(lemo$before)
# qqnorm(lemo$after)
shapiro.test(lemo$after)

# qqnorm(energy$before)
shapiro.test(energy$before)
# qqnorm(energy$after)
shapiro.test(energy$after)
```
Assumption for normality met since all four both p-values are larger than 0.05.

Check run time difference between before and after drink:
```{r}
t.test(lemo$before, lemo$after, paired = TRUE, var.equal = FALSE)
t.test(energy$before, energy$after, paired = TRUE, var.equal = FALSE)
```
p > 0.05, thus there is no evidence for a difference between before and after drink run times for both the energy and the softdrink group.


c) For each pupil compute the time difference between the two running tasks. Test whether these time
differences are effected by the type of drink.

Calculate differences, check for normality and then test whether these time differences are effected by the type of drink:
```{r}
lemo_diff = lemo$before - lemo$after
energy_diff = energy$before - energy$after

# qqnorm(lemo_diff)
shapiro.test(lemo_diff)
# qqnorm(energy_diff)
shapiro.test(energy_diff)

t.test(lemo_diff, energy_diff, paired = FALSE)
```
Assumption for normality met since both p-values are larger than 0.05.
The limo and energy time differences show no evidence for a true difference since the t-test p > 0.05

d) Can you think of a plausible objection to the design of the experiment in b) if the main aim was to test
whether drinking the energy drink speeds up the running? Is there a similar objection to the design
of the experiment in c)? Comment on all your findings in this exercise.

Experiment b takes the before drink times as control group and the after drink as experiment group, whereas the aim of the experiment was to investigate the difference between energy drink an limo on running times after the drink. When investigating a difference between before and after drinking the difference between drinks is not investigated.

Furthermore, the research question states that the increase in running times after drinking energy and control group limo needs to be researched, but then only the running times after drinking need to be taken into consideration since investigating the difference between before and after drinking also includes the proven decrease in running times when performing these intensive tasks shortly after each other. This effect works against what we want to investigate. This very same point of criticism also applies to question c since that question asks for the time difference between the before and after group.


## Exercise 5. Chick weights
The dataset chickwts is a data frame included in the standard R installation, to view it, type chickwts at the R prompt. This data frame contains 71 observations on newly-hatched chicks which were randomly allocated among six groups. Each group was given a different feed supplement for six weeks, after which their weight (in grams) was measured. The data frame consists of a numeric column giving the weights, and a factor column giving the name of the feed supplement.

a) Test whether the distributions of the chicken weights for meatmeal and sunflower groups are different by performing three tests: the two samples t-test (argue whether the data are paired or not), the Mann-Whitney test and the Kolmogorov-Smirnov test. Comment on your findings.
```{r}
chickwts$feed
sun = chickwts[chickwts$feed == "sunflower",]
meat = chickwts[chickwts$feed == "meatmeal",]
sun$weight; meat$weight

t.test(sun$weight,meat$weight)
wilcox.test(sun$weight,meat$weight)

hist(sun$weight)
hist(meat$weight)

ks.test(sun$weight,meat$weight)
```
Welch two sample t-test p-value: 0.04
Mann-Whitney test p-value: 0.07
Kolmogorov-Smirnov test p-value: 0.1

b) Conduct a one-way ANOVA to determine whether the type of feed supplement has an effect on the weight of the chicks. Give the estimated chick weights for each of the six feed supplements. What is the best feed supplement?
```{r}
chicks = chickwts
sunflower = chickwts[chickwts$feed == "sunflower",]$weight
meat = chickwts[chickwts$feed == "meatmeal",]$weight
horse = chickwts[chickwts$feed == "horsebean",]$weight
linseed = chickwts[chickwts$feed == "linseed",]$weight
soy = chickwts[chickwts$feed == "soybean",]$weight
cas = chickwts[chickwts$feed == "casein",]$weight
sun; meat; horse; linseed; soy; cas

boxplot(weight ~ feed, data=chickwts)
stripchart(weight ~ feed, data=chickwts, vertical=TRUE)

chickaov = lm(weight ~ feed, data = chickwts)
anova(chickaov)
```
c) Check the ANOVA model assumptions by using relevant diagnostic tools.
```{r}

```

d) Does the Kruskal-Wallis test arrive at the same conclusion about the effect of feed supplement as the test in b)? Explain possible differences between conclusions of the Kruskal-Wallis and ANOVA tests.
```{r}
attach(chickwts)
kruskal.test(weight, feed)
```







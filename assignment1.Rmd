---
title: "Assignment 1 \n Experimental Design and Data Analysis"
output: 
  html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=3)
```

Daan Moll (VU: 2559766 / UvA: 11929332)
Ramon Bussing (VU: 2687261 / UvA: 10719482)
Karim Semin (VU: ksn222 / UvA: 11285990)


## Exercise 1

The data set birthweight.txt contains the birthweights of 188 newborn babies. We are interested in finding the underlying (population) mean μ of birthweights.

a) Check normality of the data. Compute a point estimate for μ. Derive, assuming normality (irrespective of your conclusion about normality od the data), a bounded 90% confidence interval for μ.

```{r echo=FALSE}
birth = read.table("birthweight.txt",header=TRUE)
birthweight = birth$birthweight
```

Point estimate for μ:
```{r}
mean(birthweight)
```

Check normality with Shapiro-Wilk test:
```{r}
shapiro.test(birthweight)[2]
```
p is non-significant. This implies that h0 is accepted (the data is normally distributed).


b) An expert claims that the mean birthweight is bigger than 2800, verify this claim by using a t-test. What is the outcome of the test if you take α = 0.1? And other values of α?

If we test this hypothesis with α = 0.1 we get:
```{r}
alpha = .1
t.test(birthweight, alternative = "l", conf.level = 1-alpha, mu = 2800)[3]
```

p = .9864, therefore, h0 is accepted. If we try this for alpha = .05, .01, .001, we get the following p values:
```{r}
alpha = .05
t.test(birthweight, alternative = "l", conf.level = 1-alpha, mu = 2800)[3]

alpha = .05
```



c) In the R-output of the test from b), also a confidence interval is given, but why is it different from the confidence interval found in a) and why is it one-sided?

B=1000
p=numeric(B)
for (b in 1:B) {
  x=rnorm(n,mu,sd);
  y=rnorm(m,nu,sd);
  p[b]=t.test(x,y,var.equal = TRUE)[[3]]}
a <- table(p)
length(a[names(a)<0.05])/length(a)
```
From the 1000 generated samples a fraction of 0.577 samples have p-values lower than 0.05.


## Exercise 3

a) If we plot the telephone.txt data set, we observe the following plot:
```{r echo=FALSE}
tel_data = read.table("telephone.txt",header=TRUE)
```
If we observe the data, we see several participants that have a first month bill of €0. This does not seem plausible, as the company probably would not give out subscriptions for free. Therefore, we trim the data of zero-values.

```{r}
# remove 0-values
corrected_tel_data = tel_data[apply(tel_data, 1, function(row) all(row !=0 )), ]

# plot histograms
par(mfrow=c(1,2))
hist(tel_data$Bills, xlab = "First month bills (€)", main = "data before correction", ylim = c(0,55))
hist(corrected_tel_data, xlab = "First month bills (€)", main = "data after correction", ylim = c(0,55))

B=1000
p=numeric(B)
for (i in 1:length(nus)) {
  nu = nus[i]
  for (b in 1:B) {
    x=rnorm(n,mu,sd);
    y=rnorm(m,nu,sd);
    p[b]=t.test(x,y,var.equal = TRUE)[[3]]}
  a <- table(p)
  powers[i] = length(a[names(a)<0.05])/length(a)
} 
powers_b = powers
nus_b = nus

# plot2 = plot(nus, powers)
# plot boxplots
par(mfrow=c(1,2))
boxplot(tel_data$Bills, ylab = "€", main = "data before correction")
boxplot(corrected_tel_data, ylab = "€", main = "data after correction")
```

We now see that the first bin (containing €0-€10 subscriptions) in the histogram contains a lower frequency due to the removal of zero-values. Based on the boxplots, we see that the the median of the data is relatively low with regards to the total range of subscription-prices. This implies that a larger part of participants have relatively low phone bills. Therefore, we might advice the marketing manager to focus on low-price subscriptions. 


b) We then test wether the data stems from an exponential distribution with $\lambda$ from [.01, .1] .by bootstrapping with the median as the test statistic. 

```{r}
plot(nus_a, powers_a, main="Results 2", xlab="nu", ylab="fraction of power < 0.05", pch=20, col="blue")
lines(nus_a, powers_a, xlab="nu", pch=20, col="blue")
points(nus_b, powers_b, xlab="nu", pch=20, col="red")
lines(nus_b, powers_b, xlab="nu", pch=20, col="red")
points(nus_c, powers_c, xlab="nu", pch=20, col="green")
lines(nus_c, powers_c, xlab="nu", pch=20, col="green")
legend(184, 0.5, legend=c("a", "b", "c"), col=c("blue", "red", "green"), lty=1, cex=0.8)
```
d) Explain your findings.
The graph shows the findings of question a, b and c. It is clearly shown that depending on the

difference between a and b is 30 and 100 samples, respectively.
so more samples in b, result in more fluctuations
so nu 176 gives a power of <0.1 while nu 178 gives a power of approx. 0.7.
mu = 180. so shouldnt nu 176 be a higher power? because further away from 180, so "easier" to detect that mu != nu and thus H0 false, or p < 0.05

difference between a and c is a sd of 5 and 15 respectively.

The findings of question c show that an increase in standard deviation will decrease the power of a t-test. A possible explanation for this could be that because of the higher standard deviation.   


mu is constant at 180.
n and m are same (samples)
test if mu=nu
nu ranges from 175-185

Y-axis shows the fraction of p-values < 0.05. So the amount of times were it was determined that mu does not equal nu.

a = n=m=30, mu=180 and sd=5
b = n=m=100, mu=180 and sd=5
c = n=m=30, mu=180 and sd=15

## Exercise 3
A telecommunication company has entered the market for mobile phones in a new country. The company’s marketing manager conducts a survey of 200 new subscribers for mobile phones. The results of the survey are in the data set telephone.txt, which contains the first month bills X1 , . . . X200 , in euros.

a) Make an appropriate plot of this data set. What marketing advice(s) would you give to the marketing manager? Are there any inconsistencies in the data? If so, try to fix these.
lambdas = seq(.01, .1, .0005)
t = median(corrected_tel_data)
B=1000
tstar=numeric(B)
n=length(corrected_tel_data)
p_values = c()

for(lamb in lambdas){
  for (i in 1:B){
    xstar=rexp(n, lamb)
    tstar[i]=median(xstar)}
    
  p_left=sum(tstar<t)/B
  p_right=sum(tstar>t)/B
  p = 2*min(p_left,p_right)
  p_values = c(p_values, p)
}

plot(lambdas, p_values, main = "P-values for lambda = [.01, 1]", xlab="lambda", ylab="p-values", pch=20)

max_index_lamb = which.max(p_values)
estimated_lamb= lambdas[max_index_lamb]

cat("p-value for lambda =", estimated_lamb,"-->", max(p_values))
```

In the graph, we find a peak for $\lambda = .024$ with $p = .97$  This implies that the distribution of the data comes from an exponential distribution with $\lambda = .024$. 

b) By using a bootstrap test with the test statistic T = median(X1,...,X200), test whether the data telephone.txt stems from the exponential distribution Exp(λ) with some λ from [0.01, 0.1].
```{r}
lamb = .05
hist(corrected_telephone_data, prob=T, ylim = c(0,0.05))
x = seq(0, length(corrected_telephone_data),length=1000)
f = lamb*exp(-lamb*x )
lines(x,f,col="blue",lwd=2)
```

```{r}
t = median(corrected_telephone_data)
B=1000
tstar=numeric(B)
n=length(corrected_telephone_data)
for (i in 1:B){
  xstar=rexp(n, lamb)
  tstar[i]=median(xstar)}
  hist(tstar,prob=T)
  
p_left=sum(tstar<t)/B
p_right=sum(tstar>t)/B
p=2*min(p_left,p_right)
p_left;p_right;p
```

The p-value is .002, therefore, h0 is rejected. This means that the distribution of the data does not come from the exponential distribution with lambda = 1/median(data). 

c) Construct a 95% bootstrap confidence interval for the population median of the sample.

c) Next, we constract a 95% bootstrap confidence interval for the population median of the sample. This is done by iteratively resampling sub-datasets from the original dataset with replacement. We do this 5000 times, in order to get robust results.

```{r}
set.seed(10)
B=5000
Tstar=numeric(B)
T1 = median(corrected_tel_data)

for(i in 1:B) {
  Xstar=sample(corrected_tel_data,replace=TRUE)
  Tstar[i]=median(Xstar) }
  Tstar025=quantile(Tstar,0.025)
  Tstar975=quantile(Tstar,0.975)
  
population_median = mean(Tstar)
cat('95% CI = [',2*T1-Tstar975, '-',2*T1-Tstar025,'] with population median:', population_median)
```
The 95% bootstrap confidence interval for the population median of the telephone data is [16.4, 37.7] around the population median (28.7).


d) Assuming X1 , . . . Xn ∼ Exp(λ) and using the central limit theorem for the sample mean, estimate λ and construct again a 95% confidence interval for the population median. Comment on your findings.

First we will repeat the steps from question 1c, but with the sample mean in stead of the sample medians. 

```{r}
B=5000
Tstar=numeric(B)

for(i in 1:B) {
  Xstar=sample(corrected_tel_data,replace=TRUE)
  Tstar[i]=mean(Xstar) 
  }
Tstar025=quantile(Tstar,0.025)
Tstar975=quantile(Tstar,0.975)

# we use the estimated median from 3b as the population median
c(2*population_median-Tstar975,2*population_median-Tstar025)
```


If we plot the means of the data, we get:
```{r}
hist(Tstar, prob=T)
```

We then check whether the data is normally distributed:
```{r}
qqnorm(Tstar)
```

Therefore, the CLT applies. We can then obtain an estimate for $\lambda$ with:
$\lambda = \frac{1}{\mu}$

Giving us an estimate of:
```{r}
error = qnorm(0.975)*sd(Tstar)/sqrt(length(Tstar))
left = mean(Tstar) - error
right = mean(Tstar) + error

cat('95% CI =[',left,right,']')


est_lamb = 1/(mean(Tstar))
est_lamb


```

If we plot the exponential distribution with $\lambda = .022$ against our data, we observe:
```{r}
hist(corrected_tel_data, prob=T, ylim = c(0,0.05))
x = seq(0, length(corrected_tel_data),length=1000)
f = est_lamb*exp(-est_lamb*x )
lines(x,f,col="blue",lwd=2)
```




e) Using an appropriate test, test the null hypothesis that the median bill is bigger or equal to 40 euro against the alternative that the median bill is smaller than 40 euro. Next, design and perform a test
to check whether the fraction of the bills less than 10 euro is less than 25%.


An appropriate test to test the null hypothesis of a median greater than some value is the Wilcoxon test [leg uit wat de test doet]. 
```{r}
wilcox.test(corrected_tel_data, mu=40, alternative="less")
```

We get a p-value of 1, implying that we should not reject H0. 

Next, we test whether the fraction of the bills less than 10 euro is less than 25%. We can do this by bootstrapping a 25% confidence interval with a test statistic of 10.

```{r}
B=5000
Tstar=numeric(B)
T1 = 10

for(i in 1:B) {
  Xstar=sample(corrected_tel_data,replace=TRUE)
  Tstar[i]=sum(Xstar < T1) / length(Xstar)
  }
Tstar95=quantile(Tstar,0.95)
Tstar95
```

Based on the one-sided 95% confidence interval, we can not conclude that 25% of the bills are less than 10 euro.[uitwerken]



## Exercise 4
To study the effect of energy drink a sample of 24 high school pupils were randomized to drinking either a
softdrink or an energy drink after running for 60 meters. After half an hour they were asked to run again.
For both sprints they were asked to sprint as fast they could, and the sprinting time was measured. The
data is given in the file run.txt. [Courtesy class 5E, Stedelijk Gymnasium Leiden, 2010.]

```{r echo=FALSE}
# Load data
run = read.table("run.txt",header=TRUE)
lemo = subset(run, drink == "lemo")
energy = subset(run, drink == "energy")
```

a) Disregarding the type of drink, test whether the run times before drink and after are correlated.

Check for normality:
```{r}
# qqnorm(run$before, main = "Normal Q-Q plot before drink")
# qqnorm(run$after, main = "Normal Q-Q plot after drink")
shapiro.test(run$before)
shapiro.test(run$after)
```
Assumption for normality met since both p-values are larger than 0.05.

```{r}
plot(run$before, run$after, xlab="Run time before drink", ylab="Run time after drink")
cor.test(run$before, run$after,  method = "pearson")
```

Since Pearson's product-moment correlation p < 0.05 we can state that we found significant evidence that a true correlation exists.

b) Test separately, for both the softdrink and the energy drink conditions, whether there is a difference in speed in the two running tasks.

Check for normality:
```{r}
# qqnorm(lemo$before)
shapiro.test(lemo$before)
# qqnorm(lemo$after)
shapiro.test(lemo$after)

# qqnorm(energy$before)
shapiro.test(energy$before)
# qqnorm(energy$after)
shapiro.test(energy$after)
```
Assumption for normality met since all four both p-values are larger than 0.05.

Check run time difference between before and after drink:
```{r}
t.test(lemo$before, lemo$after, paired = TRUE, var.equal = FALSE)
t.test(energy$before, energy$after, paired = TRUE, var.equal = FALSE)
```
p > 0.05, thus there is no evidence for a difference between before and after drink run times for both the energy and the softdrink group.


c) For each pupil compute the time difference between the two running tasks. Test whether these time
differences are effected by the type of drink.

Calculate differences, check for normality and then test whether these time differences are effected by the type of drink:
```{r}
lemo_diff = lemo$before - lemo$after
energy_diff = energy$before - energy$after

# qqnorm(lemo_diff)
shapiro.test(lemo_diff)
# qqnorm(energy_diff)
shapiro.test(energy_diff)

t.test(lemo_diff, energy_diff, paired = FALSE)
```
Assumption for normality met since both p-values are larger than 0.05.
The limo and energy time differences show no evidence for a true difference since the t-test p > 0.05

d) Can you think of a plausible objection to the design of the experiment in b) if the main aim was to test
whether drinking the energy drink speeds up the running? Is there a similar objection to the design
of the experiment in c)? Comment on all your findings in this exercise.

Experiment b takes the before drink times as control group and the after drink as experiment group, whereas the aim of the experiment was to investigate the difference between energy drink an limo on running times after the drink. When investigating a difference between before and after drinking the difference between drinks is not investigated.

Furthermore, the research question states that the increase in running times after drinking energy and control group limo needs to be researched, but then only the running times after drinking need to be taken into consideration since investigating the difference between before and after drinking also includes the proven decrease in running times when performing these intensive tasks shortly after each other. This effect works against what we want to investigate. This very same point of criticism also applies to question c since that question asks for the time difference between the before and after group.


## Exercise 5. Chick weights
The dataset chickwts is a data frame included in the standard R installation, to view it, type chickwts at the R prompt. This data frame contains 71 observations on newly-hatched chicks which were randomly allocated among six groups. Each group was given a different feed supplement for six weeks, after which their weight (in grams) was measured. The data frame consists of a numeric column giving the weights, and a factor column giving the name of the feed supplement.

a) Test whether the distributions of the chicken weights for meatmeal and sunflower groups are different by performing three tests: the two samples t-test (argue whether the data are paired or not), the Mann-Whitney test and the Kolmogorov-Smirnov test. Comment on your findings.
```{r}
# chickwts$feed
sun = chickwts[chickwts$feed == "sunflower",]$weight
meat = chickwts[chickwts$feed == "meatmeal",]$weight
# sun$weight; meat$weight

# hist(sun)
# hist(meat)

#check for normality, p > 0.05 means no reason to assume non-normal data.
shapiro.test(sun)
shapiro.test(meat)

t.test(sun,meat, paired = FALSE)
wilcox.test(sun,meat)
ks.test(sun,meat)
```
Welch two sample t-test p-value: 0.04
Mann-Whitney test p-value: 0.07
Kolmogorov-Smirnov test p-value: 0.1

Since the chickens in both groups are unique the data is not paired. Paired data would be the case if we would measure the same chicken twice. The Welch's T-test gives a p-value of 0.04, hence rejecting H0 according to the common alpha=0.05, meaning that we have reason to assume that the true mean weights of the chickens with sunflower and meatmeal are different. 

The Mann-Whitney test has a p > 0.05, therefore we can not conclude from this test that the populations are unequal. Given that this is a non-parametric test it has less power than the parametric t-test, we therefore expect this p-value to be higher than the t-test. This test bases its p-value on the discrepancy between the mean ranks of ordered groups.

The KS-test has a p < 0.05, therefore we can conclude that the populations are unequal. This test bases its p-value on the largest discrepancy between two cummulative density frequency distributions from the two samples. 


b) Conduct a one-way ANOVA to determine whether the type of feed supplement has an effect on the weight of the chicks. Give the estimated chick weights for each of the six feed supplements. What is the best feed supplement?
```{r}
chickwts = chickwts

boxplot(weight ~ feed, data=chickwts,las=2)
# stripchart(weight ~ feed, data=chickwts, vertical=TRUE,las=2)

chickaov = lm(weight ~ feed, data = chickwts)
anova(chickaov)
```
Given that the ANOVA only tells us that at least one of all the tested groups are different from the rest, thus no conclusions about the best feed supplement can be made. In addition to this, "best" is undefined in the question, meaning that we don't know what best means for the researcher. Maybe chickens with less weight variance can be sold more easily and are thus the "best", but maybe the highest mean weight can be sold for the most money and is thus the best. We simply don't know, therefore we can't state which group is the best with only the information given.

c) Check the ANOVA model assumptions by using relevant diagnostic tools.
```{r}
plot(chickaov, 1)
```
This plot does not show a relationship between the fitted values and the residuals, allowing us to conclude that the assumption for equality of variances is met.

```{r}
plot(chickaov, 2)
shapiro.test(residuals(chickaov))
```
This plot shows that the measurements follow the diagonal dotted line, allowing us to conclude that the assumption for normality is met. This claim is supported by the Shapiro–Wilk test since its p-value is higher than the common alpha of 0.05.


d) Does the Kruskal-Wallis test arrive at the same conclusion about the effect of feed supplement as the test in b)? Explain possible differences between conclusions of the Kruskal-Wallis and ANOVA tests.

The ANOVA gives a p-value that states the probability that the means of three or more groups are equal. When the p-value is considered significant, then we can conclude that the mean of at least one group is different from the others.
The Kruskal-wallis test is also able to compare distributions, but than for two groups rather than three or more. In addition to this, the Kruskal-wallis test bases its p-value on the largest discrepancy between two cummulative density frequency distributions from the two samples rather than the means.






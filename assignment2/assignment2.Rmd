---
title: "Assignment 2 EDDA"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, strip.white = TRUE)
options(digits=3)
```

Daan Moll (VU: 2559766 / UvA: 11929332)
Ramon Bussing (VU: 2687261 / UvA: 10719482)
Karim Semin (VU: ksn222 / UvA: 11285990)

Group number: 49

For all exercises, an $\alpha$ significance level of .05 is used, unless stated otherwise.


### Exercise 1: Moldy bread

**1a)** 
Firstly the bread.txt file is loaded in.

```{r}
bread = read.table("bread.txt",header=TRUE)
```

The randomization process of the data is done as followed:
```{r}
I=3; J=2; N=3
rbind(rep(1:I,each=N*J),rep(1:J,N*I),sample(1:(N*I*J)))
```
Here, you can see that all the participants (in this case, slices of bread) are assigned a random condition. In the table above, the first row contains the environment (1 = cold, 2 = intermediate, 3 = warm), the second row contains the humidity (1 = dry, 2 = wet) and the third row contains the participant ID's (the specific slice of bread). The last row is thus randomized across the different conditions.

**1b)** 
Next, we create two boxplots of the hours to decay in relation to the two factors.

```{r message=FALSE, warning=FALSE}
attach(bread)
par(mfrow=c(1,2), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
boxplot(hours~environment)
boxplot(hours~humidity)
```
In the left boxplot we observe the distribution of the dependent variable (hours to decay) for the three environment conditions. Here, it can be observed that the median for the intermediate and warm environment is similar. The median for the bread in the cold environment however is higher than the other two conditions. This probably implies that placing bread in a cold environment will take longer to become moldy. 

In the right plot, the distribution of data is plotted for the humidity condition. Here, it can be seen that the median for the hours to decay is lower for wet humidity than for dry humidity. Moreover, a larger spread in data is observed for the wet environment than for the dry environment. 

It should be noted that these boxplots only give us an idea of the main effects of the factors can be observed in the boxplot. Therefore interaction plots for both factors are added below.
```{r}
layout(matrix(c(1,2,3,3), ncol=2, byrow=TRUE), heights=c(2, 1))
interaction.plot(environment, humidity, hours)
interaction.plot(humidity, environment, hours)
```
In the plots it can be noted that the lines are not parallel. In the left interaction plotm, we note that in the wet condition, the temperature looks to have a larger effect on the hours of decay than in the dry condition. In the right interaction plot, we find a higher hours to decay in the wet condition for the cold environment than in the dry condition. For the intermediate and warm environment, a vice versa effect is found; the mean hours to decay is lower in the wet condition than in the dry condition.  Thus, an interaction effect is found. However, noise might also be the cause of dissimilarities in the line slopes. To further investigate the significance of interaction between factors, a two way ANOVA is appropriate. 

**1c)** 
A two way ANOVA is performed to test the effect of the factors and their interaction.

We define the alternative hypothesis as followed:
- There is an effect of environment's temperature on the decay of bread ($H_A$)
- There is an effect of humidity on the decay of bread ($H_B$)
- There is an interaction effect between the environments temperature and humidity on the decay of bread ($H_{AB}$)

To research these hypotheses, we need to determine the $F_A$, $F_B$ and $F_{AB}$.
```{r}
bread$environment=as.factor(bread$environment)
bread$humidity=as.factor(bread$humidity)

breadov=lm(hours~environment*humidity)
anova(breadov)
```
The p-value for $H_0$ for both factors $A$ and $B$ is <0.001, which is lower than our significance value \alpha (.05). From this, we can conclude that the variation between different environments is larger than within environments. The same can be concluded for humidity; the variation between a dry and wet environment is higher than within those two environments. Therefore, a main effect is found for both variables and we may accept $H_A$ that there is a significant relationship between a bread's environment and its decay, as well as for $H_B$; there is a significant relationship between the humidity and decay of bread (within our confidence interval).

The p-value for the interaction effect between the factors $AB$ too is <0.001. From this, we may accept the alternative hypothesis $H_{AB}$ that there is a interaction effect between the environment's temperature and humidity on the decay of the bread. This confirms our findings in the interaction plot in 1c), where we observed that the lines are not parallel to each other.

**1d)** 
To answer the question of which of the two factors has the greatest (numerical) influence on the decay, the additive model is generated:
```{r}
breadov1=lm(hours~environment+humidity)
anova(breadov1)
```

Based on the additive ANOVA model of the two factors, it can be noted that the $F_A$ (23.11) is higher than $F_B$ (6.16). From this, we may conclude that more of the variance is explained by the environment's temperature than by humidity within their respective groups. Therefore, environment might have a greater influence on the decay than humidity. However, since an interaction effect was found between the two factors, this conclusion cannot be made. Since an interaction effect was found, the relationship between the environment/humidity and hours to decay may be influenced by a third factor. Therefore "which of the two factors has the greatest (numerical) influence on the decay" is not a good question in this case.

**1e)** 
Lastly, we check the model assumptions using relevant diagnostic tools. These assumptions consist of normality and equal variances of the residuals.
```{r}
par(mfrow=c(1,2), mar=c(4,4,4,1), oma=c(0.4,0.4,0.4,0))
plot(breadov,2)
plot(breadov,1)
```
In the Q-Q plot, normality looks dubious. Moreover, the spread of variance looks slightly unstable for some fitted values, which implies that there may be a relationship between residuals and fitted values. Therefore, homogeneity of variance cannot be assumed. This is caused by some of the data points (marked with 5, 7 and 8), which look to be extreme. These datapoints also have an affect on the normality of the data. We may consider these outliers and trim the data of these values.

### Exercise 2: Search engine
```{r}
students = read.table("search.txt",header=TRUE)
```

**2a)** Firstly, we randomize the students to the interfaces in a randomized block design.
```{r echo=FALSE}
I = 3
B = 5
N = 1
for (b in 1:B) {
  print(sample(1:(N*I)))
}
```
The 5 different blocks (rows) show how the experiment could be conducted by using the randomized block design. Each block is one of the 5 skill levels and it tells us how the 3 students of the skill level should be divided over the 3 interfaces. For example, for block (skill level) 1 student 2 is assigned to interface 3, student 3 is assigned to interface 1 and student 1 to interface 2.

**2b)** Then we test the null hypothesis that the search time is the same for all interfaces. 
```{r message=FALSE, warning=FALSE}
students$interface = as.factor(students$interface)
students$skill = as.factor(students$skill)
attach(students)

par(mfrow=c(1,2))
boxplot(time~interface)
boxplot(time~skill)
studentslm = lm(time~skill+interface)
anova(studentslm)
```
In the boxplot, it can be observed that the medians for all interfaces and skill levels differ from each other. The significance of these difference can be found in the output of the two-way ANOVA. This gives us a p-value for interface of .013 this is below the alpha significance level of .05. This means that we reject the null hypothesis that the search time is the same for all interfaces.

Next, we examine which type of interface requires the longest search time.
```{r}
inter1 = mean(students$time[interface==1])
inter2 = mean(students$time[interface==2])
inter3 = mean(students$time[interface==3])
which.max(c(inter1, inter2, inter3))
```
We find that interface 3 has the highest mean search time.

Moreover, we explore which combination of skill and interface has the shortest search time.
```{r}
students[which.min(students$time),]
```
The combination of skill and interface that leads to the shortest search time is skill level 2 and interface 1.

The time it takes a typical user of skill level 3 to find the product on the website if the website uses interface 3 is then estimated.
```{r}
summary(studentslm)
```
From the summary, we can calculate the answer to our question. This results in the following formula:
time = 15.01 + 4.46 + 3.03
This results in the estimation that it takes a user of skill level 3, 22.5 to find a product on the website when using interface 3.

**2c)** Then, we check the model assumptions by using relevant diagnostic tools.
```{r}
par(mfrow=c(1,2), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
plot(studentslm,2)
plot(studentslm,1)
```
The Q-Q plot seems to deviate from normal. Furthermore, the spread of variance looks slightly unstable for some fitted values, which implies that there may be a relationship between residuals and fitted values. Therefore, homogeneity of variance is questionable.

**2d)** Then, we perform the Friedman test to test whether there is an effect of the interface on search time.
```{r}
interaction.plot(students$interface, students$skill, students$time)
friedman.test(time,interface,skill,data=students)
```
The Friedman test gives us a p-value of .04. This is smaller than the significance level alpha (.05), therefore we reject H0, implying that the interface has an effect on the search time.

**2e)**
Lastly, we test the null hypothesis that the search time is the same for all interfaces with a one-way ANOVA test, ignoring the variable skill.
```{r}
studentsaov = lm(time ~ interface, data=students)
anova(studentsaov)
```
The p-value > .05, therefore we would not reject $H_0$, meaning that we don't find a significant difference between interfaces. It may be incorrect to use a one-way ANOVA test, because the skill variable may not be spread equally (uniformly) across the population of people who would use the interfaces, meaning that it may not represent the society. Therefore, if only interface would be tested, a random (and preferable larger) sample should be taken from the population. 

### Exercise 3. Feedingstuffs for cows
**3a)** We first test whether the type of feedingstuffs influences milk production using an ordinary “fixed effects” model, fitted with lm. To do this, we first read in the data and define the different factors.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(lme4)
# load data
cow=read.table("cow.txt",header=TRUE)
cow$treatment=factor(cow$treatment)
cow$id=factor(cow$id)
cow$per=factor(cow$per)
cow$order=factor(cow$order)
attach(cow)
```

Then, an ANOVA is performed for the different factors.

```{r}
anova(lm(milk~id+per+treatment))
```
From this model we can't conclude that feedingstuffs A and B have an effect on milk production with the fixed effect model since the corresponding ANOVA p-value is 0.517.

```{r}
summary((lm(milk~id+per+treatment)))
```
In the given data Feedingstuff B is estimated to give 0.510 times less milk than feedingstuff A (that has an estimate of 30.30 milk production).


Check assumptions:
```{r}
par(mfrow=c(1,2), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
plot(lm(milk~id+per+treatment), 1)
plot(lm(milk~id+per+treatment), 2)
```
This plot does show a relationship between the fitted values and the residuals as the data does not follow the dotted line, allowing us to conclude that the assumption for equality of variances is not met. In addition to this, the assumption for normality is also not met as the data in the normal Q-Q plot does not follow the dotted line.

**3b)** We repeat a) by performing a mixed effects analysis, modeling the cow effect as a random effect (use the function lmer). We compare our results to the results found by using a fixed effects model.
```{r}
cowlmer = lmer(milk~order+per+treatment+(1|id))
cowlmer_reduced = lmer(milk~order+per+(1|id), REML=FALSE)
(anova(cowlmer_reduced, cowlmer))
```
In the mixed effects model the p value of 0.446 is lower than the fixed effects model but still not significant. This could have something to do with an interaction effect.


**3c)** We study the commands:
- attach(cow)
- t.test(milk[treatment=="A"],milk[treatment=="B"],paired=TRUE)
Does this produce a valid test for a difference in milk production? Is its conclusion compatible with the one obtained in a)? Why?

```{r}
t.test(milk[treatment=="A"],milk[treatment=="B"], paired=TRUE)
```

These commands do not produce a valid test for a difference in milk production because the assumptions for the t-test are not checked in these two commands. In addition to this, due to the possibility of interaction effects the paired t-test should not be used but rather the mixed effects ANOVA, which takes these into account.


```{r}
cowlmer = lmer(milk~order+treatment+per+(1|id))
cowlmer_reduced = lmer(milk~order+treatment+(1|id), REML=FALSE)
anova(cowlmer_reduced, cowlmer)
```

When we check for the period effect we indeed find that the period has significant effect with a p=0.00439. The paired t-test is not valid for our data because the two orders (8 times AB and 10 times BA) are not equally distributed over the cows and the period of measuring has a significant effect as shown before, meaning that the paired t-test not just includes the difference in milk production but also the significant period effect.


Although the p-values for the fixed model and the paired t-test are both non-significant, this conclusion is not compatible with the one obtained in a) because the paired t-test is the equivalent (gives the same p-value of 0.83) to the anova as shown below:
```{r}
anova(lm(milk~treatment+id))
```
which is not equal to the fixed model anova used in 3a that also includes the period.

### Exercise 4: Jane Austen
**4a)** Discuss whether a contingency table test for independence or for homogeneity is most appropriate here.

In order to analyze the word count of the chapters of the finished novel, we look to see whether the word count in the chapter written by her admirer are consistent with the word count written by Jane Austen's herself. We therefore test the null hypothesis that the proportion of word counts are consistent between Austin's original work and the finished work by her admirer. Because of this, a contingency table for homogeneity is appropriate. With a contigency table for homogeinity we can check whether the distribution of word counts are equal between the original work of Austen (Sense, Emma, Sand1) and her admirer (Sand2).

**4b)** Using the given data set, investigate whether Austen herself was consistent in her different novels.
Where are the main inconsistencies?

To investigate whether Austen's work was consistent in her different novels, we regard the Sense, Emma and Sand1 column of the dataset. We then calculate the sum of each row and column and the total word count. With these sums of the rows and columns and the total word count, we can calculate the expected value for each cell. These computations are done by the build in chisq.test function of R. The result of the Chi-squared test can be observed below.

```{r}
wordcounts = read.table("austen.txt", header=TRUE)
austen = subset(wordcounts, select = c('Sense', 'Emma', 'Sand1'))
z=chisq.test(austen)
z
```
The p-value is .3. Based on this, we will not reject the null hypothesis that the proportion of word counts differs significantly between Austen's book chapters. This is to be expected, as these chapters are all written by the same author. To consider inconsistencies in the rows and columns, we observe the residuals of the cells (the square root contribution of each cell to the chi-square statistic.) These values give an indication of how much cell values differ from the expected values under $H_0$.

```{r}
z$residuals
```
From the table, inconsistencies with regards to the expected wordcounts can be observed. The main inconsistencies are listed below:
- The word 'a' was used more often in Sanditon than in Sense and Sensibility book.
- The word 'without' was used much less in Emma as opposed to the other two books.
- The word 'that' was used less in Sanditon than in her other two books.


**4c)** Was the admirer successful in imitating Austen’s style? Perform a test including all data. If he was
not successful, where are the differences?

Next, we check whether the admirer successful in imitating Austen’s style. We do this by adding back the column with the admirer's word counts and once again performing a chi-square test.
```{r}
z1=chisq.test(wordcounts)
z1
```
The p-value is now smaller than .001. Based on this we will reject the null hypothesis. This implies that after adding back the Sand2 column, we can assume that the proportion of word counts are not equal between cells. We can investigate where these differences lay if we once again investigate the contribution of each to the chi-square test statistic with the residuals.

```{r}
z1$residuals
```

From the table we can gather that these differences lay;
- The admirer uses the word 'an' much more often than Austen relatively.
- The admirer uses the word 'that' far less often than Austen relatively.
- The admirer uses the word 'with' more often than Austen relatively.
(In comparison to the expected word count under $H_0$.)


### Exercise 5:
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(lme4)
expenses = read.table("expensescrime.txt",header=TRUE)
expenses$state=as.factor(expenses$state)
attach(expenses)
```

**5a)** We make some graphical summaries of the data whereafter we investigate the problem of potential and influence points, and the problem of collinearity.
```{r}
plot(expenses[,c(2,3, 4, 5, 6, 7)])
```
When looking at these scatter plots some potential points are clearly visible. An example is the data point most right in the expand-bad scatter plot.

Problem of influence points:
```{r}
print("Cook's distances for lm(expend~bad+crime+lawyers+employ+pop):")
round(cooks.distance(lm(expend~bad+crime+lawyers+employ+pop)), 2)
plot(cooks.distance(lm(expend~bad+crime+lawyers+employ+pop)),type="b", main="Cook's distances for expend~bad+crime+lawyers+employ+pop")
```
The following influence points are identified using the thumb's rule that identifies an influence point when Cook's distances is almost 1 or higher:
* data point 5 with cook's distance: 4.91
* data point 8 with cook's distance: 3.51
* data point 35 with cook's distance: 1.09
* data point 44 with cook's distance: 2.70

These data points are removed from the data:
```{r message=FALSE, warning=FALSE}
expenses = expenses[-c(5, 8, 35, 44), ]
attach(expenses)
```

Problem of collinearity:
```{r}
round(cor(expenses[,c(2, 3, 4, 5, 6, 7)]), 2)
```
From the scatter plot above and correlation values we can see that the following explanatory variables are likely to be collinear since those combinations have data points that show a linear relation and have a high correlation coefficient: pop-employ, pop-lawyers, pop-bad, employ-bad, employ-lawyers

**5b)**
step-up method
```{r}
factors = colnames(expenses)[3:length(colnames(expenses))]

# 1st factor
R2 = numeric(length(factors))
for (i in 1:length(factors)) {
  R2[i] = summary(lm(paste('expend',  '~', factors[i]), data=expenses))[[8]]
}

print("R^2 values for addition of first variable:"); factors; R2
factor_one = factors[which.max(R2)]
print(paste0("Factor with highest R^2 value: ", factor_one))

# remove chosen first factor from list
factors = factors[-which.max(R2)]

# 2nd factor
R2 = numeric(length(factors))
for (i in 1:length(factors)) {
  R2[i] = summary(lm(paste('expend',  '~', factor_one, '+', factors[i]), data=expenses))[[8]]
}

print("R^2 values for addition of second variable:"); factors; R2
factor_two = factors[which.max(R2)]
print(paste0("Factor with highest R^2 value: ", factor_two))

summary(lm(expend~employ), data=expenses)
```
The step up method resulted in a model which used only the variable employ and a R^2 value of 0.956. The addition of a second parameter, as shown above, would only increase the R^2 value to a maximum of 0.965. However, this small increase is not worth the addition of another variable and thus the final model used one variable. This resulted in:
expend = 23.242 + 0.037 * employ + error
with an R^2 value of 0.956

step-down method
```{r echo=FALSE}
# start with all variables
summary(lm(expend~bad+crime+lawyers+employ+pop, data=expenses))[4]
print("Crime has highest p-value of 9.86e-1, thus remove crime as variable")

summary(lm(expend~bad+lawyers+employ+pop, data=expenses))[4]
print("Pop has highest p-value of 0.270, thus remove pop as variable")

summary(lm(expend~bad+lawyers+employ, data=expenses))
print("Bad has highest p-value of 0.045, which is below 0.05")
```
All of the three left over variables, bad, lawyers and employ are smaller than 0.05, thus final model keeps these three factors. This results in:
expend = 16.5877 + 2.001 * bad + 0.020 * lawyers + 0.020 * employ + error
with a R^2 value of 0.965.
The step-up method resulted in the model that only uses one variable, employ. This resulted in the following model:
expend = 23.242 + 0.037 * employ + error
with an R^2 value of 0.956.

We prefer the model of the step-up method, because it uses one variables instead of three for a slightly lower R^2 value.

**5c)**
The model assumptions will be checked for the model that resulted from the step up method.
```{r message=FALSE, warning=FALSE}
attach(expenses)
stepuplm = lm(expend~employ)

# 2
par(mfrow=c(2,2))
plot(residuals(stepuplm), employ, main="Scatter plot against employ")

# 3
x = residuals(lm(employ~bad+crime+lawyers+pop))
y = residuals(lm(expend~bad+crime+lawyers+pop))
plot(x,y,main="Added variable plot for + Employ", xlab="residual of Employ", ylab="residual of Expend")

# 5
par(mfrow=c(2,2))
plot(residuals(stepuplm), expend)
plot(residuals(stepuplm),fitted(stepuplm))

# 6
plot(stepuplm, 2)
```
The plot of the residuals against each variable not in the model did not show any linear model and this is why they are not shown.
The qq-plot shows a non-linear line. Another step could be to try and transform the data. However, this did not result in any improvements and was thus not included.

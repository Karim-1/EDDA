---
title: "assignment2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=3)
```

Daan Moll (VU: 2559766 / UvA: 11929332)
Ramon Bussing (VU: 2687261 / UvA: 10719482)
Karim Semin (VU: ksn222 / UvA: 11285990)

Group number: 49

For all exercises, an $\alpha$ significance level of .05 is used, unless stated otherwise.


### Exercise 1: Moldy bread
If left alone bread will become moldy, rot or decay otherwise. To investigate the influence of temperature and humidity on this process, the time to decay was measured for 18 slices of white bread, which were placed in 3 different environments and humidified or not. The data are given in the file bread.txt, with the first column time to decay in hours, the second column the environment (cold, warm or intermediate temperature) and the third column the humidity.
**1a)** The 18 slices came from a single loaf, but were randomized to the 6 combinations of conditions. Present an R-code for this randomization process.

```{r}
bread = read.table("bread.txt",header=TRUE)
```

The randomization process of the data is done as followed:
```{r}
I=3; J=2; N=3
rbind(rep(1:I,each=N*J),rep(1:J,N*I),sample(1:(N*I*J)))
```


**1b)** Make two boxplots of hours versus the two factors and two interaction plots (keeping the two factors fixed in turn).
Next, we create two boxplots of the hours to decay in relation to the two factors.

```{r message=FALSE, warning=FALSE}
attach(bread)
par(mfrow=c(1,2), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
boxplot(hours~environment)
boxplot(hours~humidity)
```
TODO: interpretatie

We also create the interaction plots for both factors, which can be observed below.
```{r}
layout(matrix(c(1,2,3,3), ncol=2, byrow=TRUE), heights=c(4, 1))
interaction.plot(environment, humidity, hours)
interaction.plot(humidity, environment, hours)
```
TODO: interpretatie

**1c)** Perform an analysis of variance to test for effect of the factors temperature, humidity, and their interaction. Describe the interaction effect in words.

We define the alternative hypothesis as followed:
- There is an effect of environment's temperature on the decay of bread ($H_A$)
- There is an effect of humidity on the decay of bread ($H_B$)
- There is an interaction effect between the environments temperature and humidity on the decay of bread ($H_{AB}$)

To research these hypothesis, we need to determine the $F_A$, $F_B$ and $F_{AB}$ statistic with a two way ANOVA. 
```{r}
bread$environment=as.factor(bread$environment)
bread$humidity=as.factor(bread$humidity)

breadov=lm(hours~environment*humidity)
anova(breadov)
```
The p-value for both $H_0$ with factors $A_i =0$ and $B_j =0$ for all $i$ is <0.001. From this, we can conclude the the variation within different environments is much larger than within environments. The same can be concluded for humidity; the variation between a dry and wet environment is higher than within those two environments. Therfore a main effect is found for both variables and we may accept the alternative hypothesis that there is a significant relationship between a bread's environment and its decay, as well as between the humidity and decay of bread (within our confidence interval).

The p-value for the interaction effect between the factors $H0 :\gamma_{i,j} =0$ too is <0.001. From this, we may accept the alternative hypothesis that there is a interaction effect between the environment's temperature and humidity on the decay of the bread.

**1d)** Which of the two factors has the greatest (numerical) influence on the decay? Is this a good question?

Based on the ANOVA of the two factors, it can be noted that the $F_A$ (233.7) is higher than $F_B$ (62.3). From this, we can conclude that more of the variance is explained by the environment's temperature than by humidity within their respective groups. Therefore, environment might have a greater influence on the decay. However, since an interaction effect was found between the two factors, this conclusion can not be made. (Because of this, "which of the two factors has the greatest influence is" is not a good question.) (TODO: dit nog checken met college)

**1e)** Check the model assumptions by using relevant diagnostic tools. Are there any outliers?
Lastly, we check the model assumptions using relevant diagnostic tools. These assumptions consist of normality and equal variances of the residuals.
```{r}
qqnorm(residuals(breadov))
plot(fitted(breadov),residuals(breadov))
```
In the Q-Q plot, normality looks dubious. Moreover, the spread of variance looks relatively stable for all fitted values, which implies that there is no relationship between residuals and fitted values. Therefore, homogeneity of variance can be assumed. Nevertheless, some of the data points (marked with a red circle) look to be slightly extreme however. These may have a severely affect the normality of the data. We may consider these outliers and trim the data of these values.

# Exercise 3. Feedingstuffs for cows
In a study on the effect of feedingstuffs on lactation, a sample of nine cows were fed food of two types, and their milk production was measured. All cows were fed both types of food, during two periods, with a neutral period in-between to try and wash out carry-over effects. The order of the types of food was randomized over the cows. The observed data can be found in the file cow.txt, where A and B refer to the types of feedingstuffs.

a) Test whether the type of feedingstuffs influences milk production using an ordinary “fixed effects” model, fitted with lm. Estimate the difference in milk production.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(lme4)
# load data
cow=read.table("cow.txt",header=TRUE)
cow$treatment=factor(cow$treatment)
cow$id=factor(cow$id)
cow$per=factor(cow$per)
cow$order=factor(cow$order)
attach(cow)
# boxplot(milk~treatment)
```


Check assumptions:
```{r}
plot(lm(milk~id+per+treatment), 1)
plot(lm(milk~id+per+treatment), 2)
```
This plot does show a relationship between the fitted values and the residuals as the data does not follow the dotted line, allowing us to conclude that the assumption for equality of variances is not met. In addition to this, the assumption for normality is also not met as the data in the normal Q-Q plot does not follow the dotted line.



```{r}
print("ANOVA")
anova(lm(milk~id+per+treatment))

print("LM summary")
summary((lm(milk~id+per+treatment)))
```
From this model we can't conclude that feedingstuffs A and B have an effect on milk production with the fixed effect model since the corresponding ANOVA p-value is 0.51654.
In the given data Feedingstuff A gives 0.5100 less milk than feedingstuff B.


b) Repeat a) by performing a mixed effects analysis, modeling the cow effect as a random effect (use the function lmer). Compare your results to the results found by using a fixed effects model. (You will need to install the R-package lme4, which is not included in the standard distribution of R.)


```{r}
cowlmer = lmer(milk~order+per+treatment+(1|id))
cowlmer_reduced = lmer(milk~order+per+(1|id), REML=FALSE)
(anova(cowlmer_reduced, cowlmer))
```
In the mixed effects model the p value of 0.446 is lower than the fixed effects model but still not significant.


c) Study the commands:
- attach(cow)
- t.test(milk[treatment=="A"],milk[treatment=="B"],paired=TRUE)
Does this produce a valid test for a difference in milk production? Is its conclusion compatible with the one obtained in a)? Why?


```{r}
t.test(milk[treatment=="A"],milk[treatment=="B"],paired=TRUE)
```

These commands do not produce a valid test for a difference in milk production because the assumptions for the t-test are not checked in these two commands. In addition to this, due to the possibility of interaction effects the paired t-test should not be used but rather the mixed effects ANOVA, which takes these into account.


```{r}
cowlmer = lmer(milk~order+treatment+per+(1|id))
cowlmer_reduced = lmer(milk~order+treatment+(1|id), REML=FALSE)
(anova(cowlmer_reduced, cowlmer))
```

When we check for the sequence effect we indeed find that the sequence has significant effect with a p=0.00439. The paired t-test is not valid for our data because the two sequences (8 times AB and 10 times BA) are not equally distributed over the cows and have a significant effect as shown before, meaning that with the paired t-test not just includes the difference in milk production but also the significant sequence effect.


This conclusion is (not) compatible with the one obtained in a) because....
I think it is because with the fixed model no interactions are assumed, just like with the paired t-test.





### Exercise 4: Jane Austen
Stochastic models for word counts are used in quantitative studies on literary styles. Statistical analysis of the counts can, for example, be used to solve controversies about true authorships. Another example is the analysis of word frequencies in relation to Jane Austen’s novel Sanditon. At the time Austen died, this novel was only partly completed. Austen, however, had made a summary for the remaining part. An admirer of Austen’s work finished the novel, imitating Austen’s style as much as possible. The file austen.txt contains counts of different words in some of Austen’s novels: chapters 1 and 3 of Sense and Sensibility (stored in the Sense column), chapters 1, 2 and 3 of Emma (column Emma), chapters 1 and 6 of Sanditon (both written by Austen herself, column Sand1) and chapters 12 and 24 of Sanditon (both written by the admirer, Sand2).
**4a)** Discuss whether a contingency table test for independence or for homogeneity is most appropriate here.
In order to analyze the word count of the chapters of the finished novel, we look to see whether the word count in the chapter written by her admirer are consistent with the word count written by Jane Austen's herself. We therefore look to see whether the proportion of word counts are consistent Austin's original work and the finished work by her admirer. Because of this, a contingency table for homogeneity is appropriate, to check whether the distribution of word counts are equal between the original work of Austen (Sense, Emma, Sand1) and her admirer (Sand2).

**4b)** Using the given data set, investigate whether Austen herself was consistent in her different novels.
Where are the main inconsistencies?

To investigate whether Austen's work was consistent in her different novels, we regard the Sense, Emma and Sand1 column of the dataset. We then calculate the sum of each row and column and the total word count. With these sums of the rows and columns and the total word count, we can calculate the expected value for each cell. These computations are done by the build in chisq.test function of R. The result of the Chi-squared test can be observed below.

```{r}
wordcounts = read.table("austen.txt", header=TRUE)
austen = subset(wordcounts, select = c('Sense', 'Emma', 'Sand1'))
z=chisq.test(austen)
z
```
The p-value is .3. From this we can conclude that the word counts for each combination of word and book chapters are not independent of each other. This is to be expected, as these chapters are all written by the same Author. To consider inconsistencies in the rows and columns, we observe the residuals of the cells (the square root contribution of each cell to the chi-square statistic.) These values give an indication of how much cell values differ from the expected values under $H_0$.

```{r}
z$residuals
```
From the table, inconsistencies with regards to the expected wordcounts can be observed. The main inconsistencies are listed below:
- The word 'a' was used more often in Sanditon than in Sense and Sensibility book.
- The word 'without' was used much less in Emma as opposed to the other two books.
- The word 'that' was used less in Sanditon than in her other two books.


**4c)** Was the admirer successful in imitating Austen’s style? Perform a test including all data. If he was
not successful, where are the differences?

Next, we check whether the admirer successful in imitating Austen’s style. We do this by adding back the column with the admirer's word counts and once again performing a chi-square test.
```{r}
z1=chisq.test(wordcounts)
z1
```
The p-value is now smaller than .001, meaning that after adding back the Sand2 column, the word counts may be considered independent on each other. We can investigate where these differences lay if we once again investigate the contribution of each to the chi-square test statistic with the residuals.

```{r}
z1$residuals
```

From the table we can gather that these differences lay;
- The admirer uses the word 'an' much more often than Austen relatively.
- The admirer uses the word 'that' far less often than Austen relatively.
- The admirer uses the word 'with' more often than Austen relatively.
(In comparison to the expected word count under $H_0$.)


### Exercise 5:
The data in expensescrime.txt were obtained to determine factors related to state expenditures on criminal activities (courts, police, etc.) The variables are: state (indicating the state in the USA), expend (state expenditures on criminal activities in $1000), bad (crime rate per 100000), crime (number of persons under criminal supervision), lawyers (number of lawyers in the state), employ (number of persons employed in the state) and pop (population of the state in 1000). In the regression analysis, take expend as response variable and bad, crime, lawyers, employ and pop as explanatory variables.

```{r echo=FALSE}
library(lme4)
expenses = read.table("expensescrime.txt",header=TRUE)
expenses$state=as.factor(expenses$state)
attach(expenses)
expenses
```

**5a)** Make some graphical summaries of the data. Investigate the problem of potential and influence points, and the problem of collinearity.

problem of potential and influence points:
```{r}
model_temp = lm(expend~bad)
print("bad cook's distances:")
round(cooks.distance(model_temp), 2)
plot(expend, bad)
plot(cooks.distance(model_temp),type="b", main="Cook's distances for bad")

# model_temp = lm(expend~crime)
# print("crime cook's distances:")
# round(cooks.distance(model_temp), 2)
# plot(expend, crime)
# plot(cooks.distance(model_temp),type="b", main="Cook's distances for crime")
# 
# model_temp = lm(expend~lawyers)
# print("lawyers cook's distances:")
# round(cooks.distance(model_temp), 2)
# plot(expend, lawyers)
# plot(cooks.distance(model_temp),type="b", main="Cook's distances for lawyers")
# 
# model_temp = lm(expend~employ)
# print("employ cook's distances:")
# round(cooks.distance(model_temp), 2)
# plot(expend, employ)
# plot(cooks.distance(model_temp),type="b", main="Cook's distances for employ")
# 
# model_temp = lm(expend~pop)
# print("pop cook's distances:")
# round(cooks.distance(model_temp), 2)
# plot(expend, pop)
# plot(cooks.distance(model_temp),type="b", main="Cook's distances for pop")
```
The following influence points are identified using the thumb's rule that identifies an influence point when Cook's distances > 0.95:
- "Bad" datapoint-cook's_distance: 5-2.74, 44-8.55
- "lawyers" datapoint-cook's_distance: 5-3.69
- "employ" datapoint-cook's_distance: 5-7.00
- "pop" datapoint-cook's_distance: 5-2.23, 35-1.76

The following explanatory variables have no influence points:
- crime

To reduce the plots in this report we only show bad as example, but similar plots are used to identify the other explanatory variables using the same method. The rows 5, 35 and 44 are removed from the dataset since those have influence points.

```{r message=FALSE, warning=FALSE}
expenses = expenses[-c(5, 35, 44), ]
attach(expenses)
```


---- MOETEN WE DIT HERLALEN TOT ER GEEN INFLUENCE POINTS MEER ZIJN OF IS EEN KEER GENOEG? ----
We then repeat the process above to identify more influence points. lawyers point 7 and bad point 10 are identified as influence points and are thus removed from the dataset: 
<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- expenses = expenses[-c(7, 10), ] -->
<!-- attach(expenses) -->
<!-- ``` -->
and even more influence points emerge when re-analysing them...



Problem of collinearity:
```{r}
plot(expenses[,c(2, 3, 4, 5, 6, 7)])
round(cor(expenses[,c(2, 3, 4, 5, 6, 7)]), 2)
```
From the scatter plot and correlation values we can see that the following explanatory variables are likely to be collinear since those combinations have data points that show a linear relation and have a high correlation coefficient:
pop-employ
pop-lawyers
pop-bad
employ-bad
employ-lawyers



**b)** Fit a linear regression model to the data. Use both the step-up and the step-down method to find the best model. If step-up and step-down yield two different models, choose one and motivate your choice.


**c)** Check the model assumptions by using relevant diagnostic tools.
















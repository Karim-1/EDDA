---
title: "assignment2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=3)
```

Daan Moll (VU: 2559766 / UvA: 11929332)
Ramon Bussing (VU: 2687261 / UvA: 10719482)
Karim Semin (VU: ksn222 / UvA: 11285990)

Group number: 49

For all exercises, an $\alpha$ significance level of .05 is used, unless stated otherwise.


### Exercise 1: Moldy bread

**1a)** 
Firstly the bread.txt file is loaded in.

```{r}
bread = read.table("bread.txt",header=TRUE)
```

The randomization process of the data is done as followed:
```{r}
I=3; J=2; N=3
rbind(rep(1:I,each=N*J),rep(1:J,N*I),sample(1:(N*I*J)))
```
Here, you can see that all the participants (in this case, slices of bread) are assigned a random condition. In the table above, the first row contains the environment (1 = cold, 2 = intermediate, 3 = warm), the second row contains the humidity (1 = dry, 2 = wet) and the third row contains the participant ID's (the specific slice of bread). The last row is thus randomized across the different conditions.

**1b)** 
Next, we create two boxplots of the hours to decay in relation to the two factors.

```{r message=FALSE, warning=FALSE}
attach(bread)
par(mfrow=c(1,2), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
boxplot(hours~environment)
boxplot(hours~humidity)
```
In the left boxplot we observe the distribution of the dependent variable (hours to decay) for the three environment conditions. Here, it can be observed that the median for the intermediate and warm environment is similar. The median for the bread in the cold environment however is higher than the other two conditions. This probably implies that placing bread in a cold environment will take longer to become moldy. 

In the right plot, the distribution of data is plotted for the humidity condition. Here, it can be seen that the median for the hours to decay is lower for wet humidity than for dry humidity. Moreover, a larger spread in data is observed for the wet environment than for the dry environment. 

It should be noted that these boxplots only give us an idea of the main effects of the factors can be observed in the boxplot. Therefore interaction plots for both factors are added below.
```{r}
layout(matrix(c(1,2,3,3), ncol=2, byrow=TRUE), heights=c(3, 1))
interaction.plot(environment, humidity, hours)
interaction.plot(humidity, environment, hours)
```
In the plots it can be noted that the lines are not parallel. In the left interaction plotm, we note that in the wet condition, the temperature looks to have a larger effect on the hours of decay than in the dry condition. In the right interaction plot, we find a higher hours to decay in the wet condition for the cold environment than in the dry condition. For the intermediate and warm environment, a vice versa effect is found; the mean hours to decay is lower in the wet condition than in the dry condition.  Thus, an interaction effect is found. However, noise might also be the cause of dissimilarities in the line slopes. To further investigate the significance of interaction between factors, a two way ANOVA is appropriate. 

**1c)** 
A two way ANOVA is performed to test the effect of the factors and their interaction.

We define the alternative hypothesis as followed:
- There is an effect of environment's temperature on the decay of bread ($H_A$)
- There is an effect of humidity on the decay of bread ($H_B$)
- There is an interaction effect between the environments temperature and humidity on the decay of bread ($H_{AB}$)

To research these hypotheses, we need to determine the $F_A$, $F_B$ and $F_{AB}$.
```{r}
bread$environment=as.factor(bread$environment)
bread$humidity=as.factor(bread$humidity)

breadov=lm(hours~environment*humidity)
anova(breadov)
```
The p-value for $H_0$ for both factors $A$ and $B$ is <0.001, which is lower than our significance value \alpha (.05). From this, we can conclude the the variation between different environments is larger than within environments. The same can be concluded for humidity; the variation between a dry and wet environment is higher than within those two environments. Therefore, a main effect is found for both variables and we may accept $H_A$ that there is a significant relationship between a bread's environment and its decay, as well as for $H_B$; there is a significant relationship betwheen the humidity and decay of bread (within our confidence interval).

The p-value for the interaction effect between the factors $AB$ too is <0.001. From this, we may accept the alternative hypothesis $H_{AB}$ that there is a interaction effect between the environment's temperature and humidity on the decay of the bread. This confirms our findings in the interaction plot in 1c), where we observed that the lines are not parallel to each other.

**1d)** 

To answer the question of which of the two factors has the greatest (numerical) influence on the decay, the additive model is generated:
```{r}
breadov1=lm(hours~environment+humidity)
anova(breadov1)
```


Based on the ANOVA of the two factors, it can be noted that the $F_A$ (23.11) is higher than $F_B$ (6.16). From this, we may conclude that more of the variance is explained by the environment's temperature than by humidity within their respective groups. Therefore, environment might have a greater influence on the decay than humidity. However, since an interaction effect was found between the two factors, this conclusion can not be made. Since an interaction effect was found, the relationship between the environment/humidity and hours to decay may be influenced by a third factor. Therefore "which of the two factors has the greatest (numerical) influence on the decay" is not a good question in this case.

**1e)** Check the model assumptions by using relevant diagnostic tools. Are there any outliers?
Lastly, we check the model assumptions using relevant diagnostic tools. These assumptions consist of normality and equal variances of the residuals.
```{r}
plot(breadov,2)
plot(breadov,1)
```
In the Q-Q plot, normality looks dubious. Moreover, the spread of variance looks relatively stable for all fitted values, which implies that there is no relationship between residuals and fitted values. Therefore, homogeneity of variance can be assumed. Nevertheless, some of the data points (marked with 7 and 8) look to be extreme however. These may have a severely affect the normality of the data. We may consider these outliers and trim the data of these values.

### Exercise 3. Feedingstuffs for cows
In a study on the effect of feedingstuffs on lactation, a sample of nine cows were fed food of two types, and their milk production was measured. All cows were fed both types of food, during two periods, with a neutral period in-between to try and wash out carry-over effects. The order of the types of food was randomized over the cows. The observed data can be found in the file cow.txt, where A and B refer to the types of feedingstuffs.

**3a)** Test whether the type of feedingstuffs influences milk production using an ordinary “fixed effects” model, fitted with lm. Estimate the difference in milk production.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(lme4)
# load data
cow=read.table("cow.txt",header=TRUE)
cow$treatment=factor(cow$treatment)
cow$id=factor(cow$id)
cow$per=factor(cow$per)
cow$order=factor(cow$order)
attach(cow)
# boxplot(milk~treatment)
```


```{r}
anova(lm(milk~id+per+treatment))
```
From this model we can't conclude that feedingstuffs A and B have an effect on milk production with the fixed effect model since the corresponding ANOVA p-value is 0.51654.

```{r}
summary((lm(milk~id+per+treatment)))
```
In the given data Feedingstuff B is estimated to give 0.510 times less milk than feedingstuff A (that has an estimate of 30.30 milk production).


Check assumptions:
```{r}
plot(lm(milk~id+per+treatment), 1)
plot(lm(milk~id+per+treatment), 2)
```
This plot does show a relationship between the fitted values and the residuals as the data does not follow the dotted line, allowing us to conclude that the assumption for equality of variances is not met. In addition to this, the assumption for normality is also not met as the data in the normal Q-Q plot does not follow the dotted line.

**3b)** Repeat a) by performing a mixed effects analysis, modeling the cow effect as a random effect (use the function lmer). Compare your results to the results found by using a fixed effects model. (You will need to install the R-package lme4, which is not included in the standard distribution of R.)


```{r}
cowlmer = lmer(milk~order+per+treatment+(1|id))
cowlmer_reduced = lmer(milk~order+per+(1|id), REML=FALSE)
(anova(cowlmer_reduced, cowlmer))
```
In the mixed effects model the p value of 0.446 is lower than the fixed effects model but still not significant. This could have something to do with an interaction effect.


**3c)** Study the commands:
- attach(cow)
- t.test(milk[treatment=="A"],milk[treatment=="B"],paired=TRUE)
Does this produce a valid test for a difference in milk production? Is its conclusion compatible with the one obtained in a)? Why?

```{r}
t.test(milk[treatment=="A"],milk[treatment=="B"], paired=TRUE)
```

These commands do not produce a valid test for a difference in milk production because the assumptions for the t-test are not checked in these two commands. In addition to this, due to the possibility of interaction effects the paired t-test should not be used but rather the mixed effects ANOVA, which takes these into account.


```{r}
cowlmer = lmer(milk~order+treatment+per+(1|id))
cowlmer_reduced = lmer(milk~order+treatment+(1|id), REML=FALSE)
(anova(cowlmer_reduced, cowlmer))
```

When we check for the period effect we indeed find that the period has significant effect with a p=0.00439. The paired t-test is not valid for our data because the two orders (8 times AB and 10 times BA) are not equally distributed over the cows and the period of measuring has a significant effect as shown before, meaning that the paired t-test not just includes the difference in milk production but also the significant period effect.


Although the p-values for the fixed model and the paired t-test are both non-significant, this conclusion is not compatible with the one obtained in a) because the paired t-test is the equivalent (gives the same p-value of 0.83) to the anova as shown below:
```{r}
anova(lm(milk~treatment+id))
```
which is not equal to the fixed model anova used in 3a that also includes the period.






### Exercise 4: Jane Austen
Stochastic models for word counts are used in quantitative studies on literary styles. Statistical analysis of the counts can, for example, be used to solve controversies about true authorships. Another example is the analysis of word frequencies in relation to Jane Austen’s novel Sanditon. At the time Austen died, this novel was only partly completed. Austen, however, had made a summary for the remaining part. An admirer of Austen’s work finished the novel, imitating Austen’s style as much as possible. The file austen.txt contains counts of different words in some of Austen’s novels: chapters 1 and 3 of Sense and Sensibility (stored in the Sense column), chapters 1, 2 and 3 of Emma (column Emma), chapters 1 and 6 of Sanditon (both written by Austen herself, column Sand1) and chapters 12 and 24 of Sanditon (both written by the admirer, Sand2).
**4a)** Discuss whether a contingency table test for independence or for homogeneity is most appropriate here.

In order to analyze the word count of the chapters of the finished novel, we look to see whether the word count in the chapter written by her admirer are consistent with the word count written by Jane Austen's herself. We therefore test the null hypothesis that the proportion of word counts are consistent between Austin's original work and the finished work by her admirer. Because of this, a contingency table for homogeneity is appropriate. With a contigency table for homogeinity we can check whether the distribution of word counts are equal between the original work of Austen (Sense, Emma, Sand1) and her admirer (Sand2).

**4b)** Using the given data set, investigate whether Austen herself was consistent in her different novels.
Where are the main inconsistencies?

To investigate whether Austen's work was consistent in her different novels, we regard the Sense, Emma and Sand1 column of the dataset. We then calculate the sum of each row and column and the total word count. With these sums of the rows and columns and the total word count, we can calculate the expected value for each cell. These computations are done by the build in chisq.test function of R. The result of the Chi-squared test can be observed below.

```{r}
wordcounts = read.table("austen.txt", header=TRUE)
austen = subset(wordcounts, select = c('Sense', 'Emma', 'Sand1'))
austen
z=chisq.test(austen)
z
```
The p-value is .3. Based on this, we will not reject the null hypothesis that the proportion of word counts differs significantly between Austen's book chapters. This is to be expected, as these chapters are all written by the same author. To consider inconsistencies in the rows and columns, we observe the residuals of the cells (the square root contribution of each cell to the chi-square statistic.) These values give an indication of how much cell values differ from the expected values under $H_0$.

```{r}
z$residuals
```
From the table, inconsistencies with regards to the expected wordcounts can be observed. The main inconsistencies are listed below:
- The word 'a' was used more often in Sanditon than in Sense and Sensibility book.
- The word 'without' was used much less in Emma as opposed to the other two books.
- The word 'that' was used less in Sanditon than in her other two books.


**4c)** Was the admirer successful in imitating Austen’s style? Perform a test including all data. If he was
not successful, where are the differences?

Next, we check whether the admirer successful in imitating Austen’s style. We do this by adding back the column with the admirer's word counts and once again performing a chi-square test.
```{r}
z1=chisq.test(wordcounts)
z1
```
The p-value is now smaller than .001. Based on this we will reject the null hypothesis. This implies that after adding back the Sand2 column, we can assume that the proportion of word counts are not equal between cells. We can investigate where these differences lay if we once again investigate the contribution of each to the chi-square test statistic with the residuals.

```{r}
z1$residuals
```

From the table we can gather that these differences lay;
- The admirer uses the word 'an' much more often than Austen relatively.
- The admirer uses the word 'that' far less often than Austen relatively.
- The admirer uses the word 'with' more often than Austen relatively.
(In comparison to the expected word count under $H_0$.)


### Exercise 5:
The data in expensescrime.txt were obtained to determine factors related to state expenditures on criminal activities (courts, police, etc.) The variables are: state (indicating the state in the USA), expend (state expenditures on criminal activities in $1000), bad (crime rate per 100000), crime (number of persons under criminal supervision), lawyers (number of lawyers in the state), employ (number of persons employed in the state) and pop (population of the state in 1000). In the regression analysis, take expend as response variable and bad, crime, lawyers, employ and pop as explanatory variables.

```{r echo=FALSE}
library(lme4)
expenses = read.table("expensescrime.txt",header=TRUE)
expenses$state=as.factor(expenses$state)
attach(expenses)
expenses
```
**5a)** Make some graphical summaries of the data. Investigate the problem of potential and influence points, and the problem of collinearity.

```{r}
plot(expenses[,c(2,3, 4, 5, 6, 7)])
```
When looking at these scatter plots some 

```{r}
par(mfrow=c(2,3))
for (i in 2:7) {
  hist(expenses[,i], main=names(expenses)[i])
}
```


problem of potential and influence points:
```{r}
print("Cook's distances for lm(expend~bad+crime+lawyers+employ+pop):")
round(cooks.distance(lm(expend~bad+crime+lawyers+employ+pop)), 2)
plot(cooks.distance(lm(expend~bad+crime+lawyers+employ+pop)),type="b", main="Cook's distances for expend~bad+crime+lawyers+employ+pop")
```
The following influence points are identified using the thumb's rule that identifies an influence point when Cook's distances is almost 1 or higher:
* data point 5 with cook's distance: 4.91
* data point 8 with cook's distance: 3.51
* data point 35 with cook's distance: 1.09
* data point 44 with cook's distance: 2.70

These data points are removed from the data:
```{r message=FALSE, warning=FALSE}
expenses = expenses[-c(5, 8, 35, 44), ]
attach(expenses)
```


```{r}
plot(expenses[,c(2,3, 4, 5, 6, 7)])
```
The removal of the influence points is clearly visible in the plot above compared to the same plot before the removal. For example, when looking at the expend-bad scatter plot, the outlying data points are removed, resulting in a better visible relation between the expand and bad variables.


Problem of collinearity:
```{r}
round(cor(expenses[,c(2, 3, 4, 5, 6, 7)]), 2)
```
From the scatter plot above and correlation values we can see that the following explanatory variables are likely to be collinear since those combinations have data points that show a linear relation and have a high correlation coefficient:
pop-employ
pop-lawyers
pop-bad
employ-bad
employ-lawyers


**b)** Fit a linear regression model to the data. Use both the step-up and the step-down method to find the best model. If step-up and step-down yield two different models, choose one and motivate your choice.


**c)** Check the model assumptions by using relevant diagnostic tools.
















---
title: "assignment2-daan"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=3)
```

```{r}
students = read.table("search.txt",header=TRUE)
students
```
**2a** 
randomised block design
```{r echo=FALSE}
# lower the skill value the better the computer skill
# randomized block design
# numerical outcome Y
# treatment, factor of interest with I levels (type of interface)
# block, factor that is not of interest, B, (computer skill)

# purpose is to find dependence of Y on I
# block variable is thought to be of influence.
# our problem is same as example lec5slide23
I = 3
B = 5
N = 1
for (b in 1:B) {
  print(sample(1:(N*I)))
}
# maar hoe dat 15 studenten erin worden vernoemd?

# example output: 2 3 1
# means: assign student 2 to treatment 3, student 3 to treatment 1 and student 1 to treatment 2

# unit is that that receives the treatment, so student
```
**2b**
```{r echo=FALSE}
attach(students)
qqnorm(time)
shapiro.test(time)
# normality is questionable, so kruskall willis
boxplot(time)
stripchart(time~interface, vertical = TRUE)

# test null hypothesis that search time is equal for all interfaces
# if not normal i guess KW
# kruskal.test(time~interface)

# if normal then two way anova
interaction.plot(interface,skill,time)
# students$interface = as.factor(students$interface)
# students$skill = as.factor(students$skill)
studentsaov2 = lm(time~interface*skill)
anova(studentsaov2)

```
--------- If assume normal then this:
p value for interface:skill is 0.340. So there is no evidence for interaction. 


------- If assume not normal then this:
The krukal willis test was used because of questionable normality. The KW test returns a p-value of 0.1 which is greater than 0.05 and thus we accept the H0 hypothesis. This means there is no significant difference between interfaces.
// nog ff checken

What type of interface requires the longest search time?
```{r}
inter1 = mean(students$time[interface==1])
inter2 = mean(students$time[interface==2])
inter3 = mean(students$time[interface==3])
which.max(c(inter1, inter2, inter3))
```
interface 3 has the highest mean search time. TODO: probably not enough.

Which combination of skill and interface is search time shortest
```{r}

```

estimate time it takes for skill level 3 to find product with interface 3
So treatment(I) = 3, block(B)=3

Found on wiki:
estimate for B_j: barY_j - barY with barY_j


```{r}


```

**2c**
check the model assumptions by using relevant diagnostic tools
```{r}

```

**2d**
Perform the Friedman test to test whether ther is an effect of interface
```{r}
interaction.plot(students$interface, students$skill, students$time)
friedman.test(time,interface,skill,data=students)
```
The Friedman test gives us a p-value of 0.04. This is smaller than 0.05 and thus we reject the H0 hypothesis. This mean that the interface has an effect on the search time.

**2e**
test H0 that search time is the same for all interfaces by a one-way ANOVA, ignoring the variable skill. is it right/wrong or useful/not useful to perform this test on this dataset.
```{r}
studentsaov = lm(time ~ interface, data=students)
plot(studentsaov,1)
# plot(studentsaov,2)

anova(studentsaov)
summary(studentsaov)
# qqnorm(residuals(studentsaov))
# plot(fitted(studentsaov),residuals(studentsaov))
# fitted(studentsaov)
# residuals(studentsaov)
```
p-value < 0.05 thus we reject H0. There is a significant difference between interfaces. It is wrong to use a one-way ANOVA test here because the normality of the data is questionable... (MIGHT NEED MORE). 
Is it useful or not? I think not because not normal. However, Kruskal willis accepts H0, and Friedman rejects H0. Look up what this means/how to argue about this in relation to ANOVA begin useful or not.


**5**
```{r echo=FALSE}
expenses = read.table("expensescrime.txt",header=TRUE)
expenses
```

**5a**
make some graphical summaries
investigate the problem of potential and influence points and the problem of collinearity
```{r}
# attach(expenses)
# boxplot(expend~bad)
# pop
# employ
# plot(employ/(pop*1000), crime)
# boxplot(expend~crime)

# this is from slides lecture 7 slide 28.
expenses[1:3,]
# deze snap ik nog niet 100% maar ziet er vet uit
plot(expenses[,c(2,4,7)]); par(mfrow=c(1,3))
pairs(expenses[2:length(expenses)])

# expenses[1:3, c(2,4,7)]

# for (i in c(2,4,7)) {
#   hist(expenses[,i], main=names(expenses)[i])
# }

par(mfrow=c(2,3))
for (i in 2:7) {
  hist(expenses[,i], main=names(expenses)[i])
}



```
**5b**
Linear regression model to the data
expend as response variable
```{r}
expenslm = lm(expend~bad+crime+lawyers+employ+pop, data=expenses)
summary(expenslm)
```
multiple regression results in a p-value of 2e-16, this is smaller than 0.05 and thus we reject H0.
NOG FF H0 uitleggen en goed hebben wat dat precies is

step up step down
use step-up and step-down method to find the best model
if step up and step down yield two different models, choose one and motivate your choice

step up
```{r}
factors = colnames(expenses)[3:length(colnames(expenses))]
print("Starting with following factors:"); factors

# 1st factor
R2 = numeric(length(factors))

for (i in 1:length(factors)) {
  R2[i] = summary(lm(paste('expend',  '~', factors[i]), data=expenses))[[8]]
}
R2
factor_one = factors[which.max(R2)]
print(paste0("Factor with highest R^2 value: ", factor_one))

# remove chosen first factor from list
factors = factors[-which.max(R2)]
print("Factors left: "); factors


# 2nd factor
R2 = numeric(length(factors))
for (i in 1:length(factors)) {
  R2[i] = summary(lm(paste('expend',  '~', factor_one, '+', factors[i]), data=expenses))[[8]]
}
R2
factor_two = factors[which.max(R2)]
print(paste0("Factor with highest R^2 value: ", factor_two))

# remove chosen second factor from list
# factors = factors[-which.max(R2)]
# print("Factors left: "); factors


# # 3rd factor
# R2 = numeric(length(factors))
# for (i in 1:length(factors)) {
#   R2[i] = summary(lm(paste('expend',  '~', factor_one, '+', factor_two, '+', factors[i]), data=expenses))[[8]]
# }
# R2
# factor_three = factors[which.max(R2)]
# print(paste0("Factor with highest R^2 value: ", factor_three))
# 
# # remove chosen third factor from list
# factors = factors[-which.max(R2)]
# print("Factors left: "); factors
# 
# 
# # 4th factor
# R2 = numeric(length(factors))
# for (i in 1:length(factors)) {
#   R2[i] = summary(lm(paste('expend',  '~', factor_one, '+', factor_two, '+', factor_three, '+', factors[i]), data=expenses))[[8]]
# }
# R2
# factor_four = factors[which.max(R2)]
# print(paste0("Factor with highest R^2 value: ", factor_four))
# 
# last_factor = factors[-which.max(R2)]
# print(paste0("last factor left: ", last_factor))

# resulting in the model.... lecture 8 slide 11. is na employ gekozen te hebben de increase in R2, van 0.954 naar 0.963 al niet meer significant dus stoppen?

summary(lm(expend~crime), data=expenses)

```
For crime 0.954 then another variable we see lawyers is highest with 0.963. This increase is not worth it for another variable and thus the model results in:
expend = -531.039 + 0.287 * crime + error

step-down
```{r}
# start with all variables
# summary(lm(expend~bad+crime+lawyers+employ+pop, data=expenses))

# crime has highest p value of 0.2553, thus remove crime
# summary(lm(expend~bad+lawyers+employ+pop, data=expenses))

# pop has highest p value of 0.0601, thus remove pop
summary(lm(expend~bad+lawyers+employ, data=expenses))

# bad has highest p value of 0.3450, thus remove bad
summary(lm(expend~lawyers+employ, data=expenses))

# both lawyers and employ are smaller than 0.05, thus final model is line above.
# that gives:
# expend = -1.11e+02 + 2.69-02 * lawyers + 2.97e-02 * employ + error


```



**5c**
Check model assumptions by using relevant diagnostic tools.
(lecture 8, diagnostics in linear regression)
```{r}
qqnorm(residuals(expenslm))
plot(fitted(expenslm),residuals(expenslm))
```


---
title: "assignment2-daan"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=3)
```

```{r}
students = read.table("search.txt",header=TRUE)
students
```
A researcher is interested in the time it takes a student to find a certain product on the internet using a search engine. There are three different types of interfaces with the search engine and especially the effect of these interfaces is of importance. There are five different types of students, indicating their level of computer skill (the lower the value of this indicator, the better the computer skill of the corresponding student). Fifteen students are selected; three from each group with a certain level of computer skill. The data is given in the file search.txt. Assume that the experiment was run according to a randomized block design which you make in a). (Beware that the levels of the factors are coded by numbers.)
**2a** 
randomised block design
```{r echo=FALSE}
# lower the skill value the better the computer skill
# randomized block design
# numerical outcome Y
# treatment, factor of interest with I levels (type of interface)
# block, factor that is not of interest, B, (computer skill)

# purpose is to find dependence of Y on I
# block variable is thought to be of influence.
# our problem is same as example lec5slide23
I = 3
B = 5
N = 1
for (b in 1:B) {
  print(sample(1:(N*I)))
}
# maar hoe dat 15 studenten erin worden vernoemd?

# example output: 2 3 1
# means: assign student 2 to treatment 3, student 3 to treatment 1 and student 1 to treatment 2

# unit is that that receives the treatment, so student
```
dus skill level 1
student 3 interface 1
student 1 interface 2
student 2 interface 3

de 3 studenten van skill level 1:
3 1 2

de 3 studenten van skill 2 level 2:
1 2 3

**2b**
Test the null hypothesis that the search time is the same for all interfaces. 

What type of interface does require the longest search time? For which combination of skill level and type of interface is the search time the shortest? Estimate the time it takes a typical user of skill level 3 to find the product on the website if the website uses interface 3.
```{r message=FALSE, warning=FALSE}

students$interface = as.factor(students$interface)
students$skill = as.factor(students$skill)
attach(students)
interaction.plot(interface,skill,time)
studentslm = lm(time~interface*skill)
anova(studentslm)
```
--------- If assume normal then this:
p value for interface:skill is 0.340. So there is no evidence for interaction. 


------- If assume not normal then this:
The krukal willis test was used because of questionable normality. The KW test returns a p-value of 0.1 which is greater than 0.05 and thus we accept the H0 hypothesis. This means there is no significant difference between interfaces.
// nog ff checken

What type of interface requires the longest search time?
```{r}
inter1 = mean(students$time[interface==1])
inter2 = mean(students$time[interface==2])
inter3 = mean(students$time[interface==3])
which.max(c(inter1, inter2, inter3))
```
interface 3 has the highest mean search time.

Which combination of skill and interface is search time shortest
```{r}
which.min(students$time)
students[2,]
```
Interface 3 with skill 4 is lowest time (16.1 - 3.3), this

```{r}
summary(studentslm)
```

**2c**
check the model assumptions by using relevant diagnostic tools
```{r}
qqnorm(residuals(studentslm))
plot(fitted(studentslm),residuals(studentslm))
```

**2d**
Perform the Friedman test to test whether ther is an effect of interface
```{r}
interaction.plot(students$interface, students$skill, students$time)
friedman.test(time,interface,skill,data=students)
```
The Friedman test gives us a p-value of 0.04. This is smaller than 0.05 and thus we reject the H0 hypothesis. This mean that the interface has an effect on the search time.

**2e**
test H0 that search time is the same for all interfaces by a one-way ANOVA, ignoring the variable skill. is it right/wrong or useful/not useful to perform this test on this dataset.
```{r}
studentsaov = lm(time ~ interface, data=students)
plot(studentsaov,1)
# plot(studentsaov,2)

anova(studentsaov)
summary(studentsaov)
# qqnorm(residuals(studentsaov))
# plot(fitted(studentsaov),residuals(studentsaov))
# fitted(studentsaov)
# residuals(studentsaov)
```
p-value < 0.05 thus we reject H0. There is a significant difference between interfaces. It is wrong to use a one-way ANOVA test here 
because the normality of the data is questionable... (MIGHT NEED MORE). 

right wrong useful not useful
alleen de interface niet de skill level. Conclusie dat het goed is als skill levels ook uniform verdeeld zijn. Want die zijn representatief voor de samenleving/ mensen die het moeten gebruiken

Is it useful or not? I think not because not normal. However, Kruskal willis accepts H0, and Friedman rejects H0. Look up what this means/how to argue about this in relation to ANOVA begin useful or not.


**5**
```{r echo=FALSE}
expenses = read.table("expensescrime.txt",header=TRUE)
expenses
```

**5a**
make some graphical summaries
investigate the problem of potential and influence points and the problem of collinearity
```{r}
# attach(expenses)
# boxplot(expend~bad)
# pop
# employ
# plot(employ/(pop*1000), crime)
# boxplot(expend~crime)

# this is from slides lecture 7 slide 28.
expenses[1:3,]
# deze snap ik nog niet 100% maar ziet er vet uit
plot(expenses[,c(2,4,7)]); par(mfrow=c(1,3))
pairs(expenses[2:length(expenses)])

# expenses[1:3, c(2,4,7)]

# for (i in c(2,4,7)) {
#   hist(expenses[,i], main=names(expenses)[i])
# }

par(mfrow=c(2,3))
for (i in 2:7) {
  hist(expenses[,i], main=names(expenses)[i])
}



```
**5b**
step up step down
use step-up and step-down method to find the best model
if step up and step down yield two different models, choose one and motivate your choice

step up
```{r}
factors = colnames(expenses)[3:length(colnames(expenses))]

# 1st factor
R2 = numeric(length(factors))

for (i in 1:length(factors)) {
  R2[i] = summary(lm(paste('expend',  '~', factors[i]), data=expenses))[[8]]
}

print("R^2 values for addition of first variable:"); factors; R2
factor_one = factors[which.max(R2)]
print(paste0("Factor with highest R^2 value: ", factor_one))

# remove chosen first factor from list
factors = factors[-which.max(R2)]

# 2nd factor
R2 = numeric(length(factors))
for (i in 1:length(factors)) {
  R2[i] = summary(lm(paste('expend',  '~', factor_one, '+', factors[i]), data=expenses))[[8]]
}

print("R^2 values for addition of second variable:"); factors; R2
factor_two = factors[which.max(R2)]
print(paste0("Factor with highest R^2 value: ", factor_two))

summary(lm(expend~employ), data=expenses)

```
The step up method resulted in a model which used only the variable employ and a R^2 value of 0.955. The addition of a second parameter, as shown above, would only increase the R^2 value to a maximum of 0.963. However, this small increase is not worth the addition of another variable and thus the final model used one variable. This resulted in:
expend = 23.242 + 0.037 * employ + error
with an R^2 value of 0.955

step-down
```{r echo=FALSE}
# start with all variables
summary(lm(expend~bad+crime+lawyers+employ+pop, data=expenses))[4]
print("Crime has highest p-value of 0.255, thus remove crime as variable")

# crime has highest p value of 0.2553, thus remove crime
summary(lm(expend~bad+lawyers+employ+pop, data=expenses))[4]
print("Pop has highest p-value of 0.0601, thus remove pop as variable")

# pop has highest p value of 0.0601, thus remove pop
summary(lm(expend~bad+lawyers+employ, data=expenses))[4]
print("Bad has highest p-value of 0.345, thus remove bad as variable")

# bad has highest p value of 0.3450, thus remove bad
summary(lm(expend~lawyers+employ, data=expenses))
```
both lawyers and employ are smaller than 0.05, thus final model keeps these two factors. This results in:
expend = -1.11e+02 + 2.69-02 * lawyers + 2.97e-02 * employ + error
with a R^2 value of 0.963

The step up method resulted in the model that only uses one variable, employ. This resulted in the following model:
expend = 23.242 + 0.037 * employ + error
with an R^2 value of 0.955

We prefer the model of the step up method, because it uses only one variable instead of two for a slightly lower R^2 value.


**5c**
Check model assumptions by using relevant diagnostic tools.
(lecture 8, diagnostics in linear regression)

To check the model quality look at 
1. scatter plot: plot Y against each Xk separately (this yields overall picture, and shows outlying values)
2. scatter plot: plot residuals against each Xk in the model separately (look at pattern (curved?) and spread)

3. added variable plot (partial regression plot, see Velleman and Welsch (1981)): plot residuals of Xj against residuals of Y with omitted Xj (to show the effect of adding Xj to the model.) (Or, to show the relationship between Y and Xj , once all other predictors have been accounted for.)
4. scatter plot: plot residuals against each Xk not in the model separately (look at pattern — linear? then include!)
5. scatter plot: plot residuals against Y and ˆY (look at spread) 
6. normal QQ-plot of the residuals (check normality assumption)
Eduard

The model assumptions will be checked for the model that resulted from the step up method.
```{r warning=FALSE}
attach(expenses)
stepuplm = lm(expend~employ)

# 1, is in a gedaan, toch?

# 2
plot(residuals(stepuplm), employ, main="Scatter plot against employ")

# expenses$employlog = log(expenses$employ)
# expenses$employlog = 
# attach(expenses)
# stepuplmlog = lm(expend~employlog)
# plot(residuals(stepuplmlog), employlog, main="Scatter plot against employlog")


# 3
x = residuals(lm(employ~bad+crime+lawyers+pop))
y = residuals(lm(expend~bad+crime+lawyers+pop))
plot(x,y,main="Added variable plot for + Employ", xlab="residual of Employ", ylab="residual of Expend")

# 4
# par(mfrow=c(2,2))
plot(residuals(stepuplm), bad, main="Scatter plot against bad")
plot(residuals(stepuplm), crime, main="Scatter plot against crime")
plot(residuals(stepuplm), lawyers, main="Scatter plot against lawyers")
plot(residuals(stepuplm), pop, main="Scatter plot against pop")
# NIKS LINEAR DUS KAN WEG?

# 5
plot(residuals(stepuplm), expend)
plot(residuals(stepuplm),fitted(stepuplm))

# 6
qqnorm(residuals(stepuplm))
# slide 21
shapiro.test(residuals(stepuplm))
```
point 4, checked for linear patterns. No pattern found and thus plots not included.

